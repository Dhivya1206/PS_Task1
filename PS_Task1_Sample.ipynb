{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "37uLVIh_K9Xv",
        "outputId": "6241a0fa-1cff-4844-aa0f-c41d7a7ef931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed.\n",
            "Loading data...\n",
            "After removing duplicates: 97 rows\n",
            "After outlier removal: 74 rows\n",
            "Dropped 2 rows with NaN in ENTITY_LENGTH or product_size. Remaining: 72\n",
            "Training set size: 57 records, Validation set size: 15 records\n",
            "Training ideal packaging model...\n",
            "Ideal Packaging Model - Train MSE: 878940.35, Train R²: 0.08\n",
            "Results saved to 'packaging_flaw_analysis.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_59cc4814-489f-4f80-ba5b-c63aa094ccfa\", \"packaging_flaw_analysis.csv\", 4657)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 1 products with design flaws.\n",
            "Detected 2 products with packaging flaws.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from google.colab import files\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Step 1: Install Dependencies\n",
        "def install_dependencies():\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q pandas numpy scikit-learn\")\n",
        "    print(\"Dependencies installed.\")\n",
        "\n",
        "# Step 2: Load and Preprocess Data\n",
        "def preprocess_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found. Please upload the CSV file.\")\n",
        "        return None\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['ENTITY_NAME', 'ENTITY_LENGTH', 'DIMENSIONS', 'PRICE', 'BRAND'])\n",
        "    print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "    # Parse DIMENSIONS safely\n",
        "    df['dimensions'] = df['DIMENSIONS'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Impute missing DIMENSIONS with brand-specific medians, fallback to dataset-wide median\n",
        "    def impute_dimensions(group):\n",
        "        valid_dims = group['dimensions'].apply(\n",
        "            lambda x: x if isinstance(x, tuple) and all(isinstance(v, (int, float)) for v in x) else None\n",
        "        )\n",
        "        valid_dims_list = [x for x in valid_dims if x is not None]\n",
        "\n",
        "        if valid_dims_list:\n",
        "            median_dims = np.median(valid_dims_list, axis=0)\n",
        "        else:\n",
        "            all_valid_dims = df['dimensions'].apply(\n",
        "                lambda x: x if isinstance(x, tuple) and all(isinstance(v, (int, float)) for v in x) else None\n",
        "            )\n",
        "            median_dims = np.median([x for x in all_valid_dims if x is not None], axis=0)\n",
        "\n",
        "        group['dimensions'] = group['dimensions'].apply(\n",
        "            lambda x: tuple(median_dims) if x is None or not all(isinstance(v, (int, float)) for v in x) else x\n",
        "        )\n",
        "        return group\n",
        "\n",
        "    df = df.groupby('BRAND').apply(impute_dimensions, include_groups=False).reset_index()\n",
        "\n",
        "    # Compute product_size\n",
        "    df['product_size'] = df['dimensions'].apply(\n",
        "        lambda x: x[0] * x[1] * x[2] if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Compute box_volume (add 10% padding to each dimension)\n",
        "    df['box_volume'] = df['dimensions'].apply(\n",
        "        lambda x: (x[0] * 1.1) * (x[1] * 1.1) * (x[2] * 1.1) if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Filter outliers in product_size (relaxed to 2*IQR)\n",
        "    Q1 = df['product_size'].quantile(0.25)\n",
        "    Q3 = df['product_size'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[df['product_size'].between(Q1 - 2 * IQR, Q3 + 2 * IQR)]\n",
        "    print(f\"After outlier removal: {len(df)} rows\")\n",
        "\n",
        "    # Drop rows with NaN in ENTITY_LENGTH or product_size\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['ENTITY_LENGTH', 'product_size'])\n",
        "    print(f\"Dropped {original_len - len(df)} rows with NaN in ENTITY_LENGTH or product_size. Remaining: {len(df)}\")\n",
        "\n",
        "    if len(df) < 30:\n",
        "        print(f\"Error: Only {len(df)} valid records available. Need at least 30.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Train Ideal Packaging Model\n",
        "def train_ideal_packaging_model(X_train, y_train):\n",
        "    print(\"Training ideal packaging model...\")\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    r2_train = r2_score(y_train, y_pred_train)\n",
        "    print(f\"Ideal Packaging Model - Train MSE: {mse_train:.2f}, Train R²: {r2_train:.2f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 4: Detect Flaws\n",
        "def detect_flaws(df_test, ideal_model):\n",
        "    X_test = df_test[['ENTITY_LENGTH']].values\n",
        "    df_test['ideal_box_volume'] = ideal_model.predict(X_test)\n",
        "    df_test['ideal_product_size'] = df_test['ideal_box_volume'] * 0.9\n",
        "\n",
        "    # Detect flaws\n",
        "    df_test['design_flaw'] = df_test['product_size'] > 2 * df_test['ideal_product_size']\n",
        "    df_test['packaging_flaw'] = df_test['box_volume'] > 1.5 * df_test['ideal_box_volume']\n",
        "\n",
        "    # Excess volume calculation\n",
        "    df_test['excess_volume'] = df_test.apply(\n",
        "        lambda row: row['box_volume'] - row['ideal_box_volume'] if row['packaging_flaw'] else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Recommendation\n",
        "    df_test['recommendation'] = df_test.apply(\n",
        "        lambda row: (\n",
        "            f\"Reduce box to {row['ideal_box_volume']:.1f} cm³ (save {row['excess_volume']:.1f} cm³)\"\n",
        "            if row['packaging_flaw']\n",
        "            else \"Redesign bottle to reduce volume\" if row['design_flaw']\n",
        "            else \"No flaw detected\"\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df_test\n",
        "\n",
        "# Step 5: Main Execution\n",
        "def main():\n",
        "    install_dependencies()\n",
        "\n",
        "    file_path = \"/content/amazon_shampoo_products.csv\"\n",
        "    df = preprocess_data(file_path)\n",
        "\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Split data\n",
        "    df_train, df_valid = train_test_split(df, train_size=0.8, test_size=0.2, random_state=42)\n",
        "    print(f\"Training set size: {len(df_train)} records, Validation set size: {len(df_valid)} records\")\n",
        "\n",
        "    # Train model\n",
        "    X_train = df_train[['ENTITY_LENGTH']].values\n",
        "    y_train = df_train['box_volume']\n",
        "    ideal_model = train_ideal_packaging_model(X_train, y_train)\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid = detect_flaws(df_valid, ideal_model)\n",
        "\n",
        "    # Save results\n",
        "    output_df = df_valid[[\n",
        "        'ENTITY_ID', 'ENTITY_NAME', 'ENTITY_LENGTH', 'product_size', 'box_volume',\n",
        "        'ideal_product_size', 'ideal_box_volume', 'design_flaw', 'packaging_flaw',\n",
        "        'excess_volume', 'recommendation'\n",
        "    ]]\n",
        "    output_file = \"packaging_flaw_analysis.csv\"\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"Results saved to '{output_file}'.\")\n",
        "    files.download(output_file)\n",
        "\n",
        "    # Summary\n",
        "    design_flaws = df_valid['design_flaw'].sum()\n",
        "    packaging_flaws = df_valid['packaging_flaw'].sum()\n",
        "    print(f\"Detected {design_flaws} products with design flaws.\")\n",
        "    print(f\"Detected {packaging_flaws} products with packaging flaws.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from google.colab import files\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Step 1: Install Dependencies\n",
        "def install_dependencies():\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q pandas numpy scikit-learn\")\n",
        "    print(\"Dependencies installed.\")\n",
        "\n",
        "# Step 2: Load and Preprocess Data\n",
        "def preprocess_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found. Please upload the CSV file.\")\n",
        "        return None\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['ENTITY_NAME', 'ENTITY_LENGTH', 'DIMENSIONS', 'PRICE', 'BRAND'])\n",
        "    print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "    # Parse DIMENSIONS safely\n",
        "    df['dimensions'] = df['DIMENSIONS'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Impute missing DIMENSIONS with brand-specific medians, fallback to dataset-wide median\n",
        "    def impute_dimensions(group):\n",
        "        valid_dims = group['dimensions'].apply(\n",
        "            lambda x: x if isinstance(x, tuple) and all(isinstance(v, (int, float)) for v in x) else None\n",
        "        )\n",
        "        valid_dims_list = [x for x in valid_dims if x is not None]\n",
        "\n",
        "        if valid_dims_list:\n",
        "            median_dims = np.median(valid_dims_list, axis=0)\n",
        "        else:\n",
        "            all_valid_dims = df['dimensions'].apply(\n",
        "                lambda x: x if isinstance(x, tuple) and all(isinstance(v, (int, float)) for v in x) else None\n",
        "            )\n",
        "            median_dims = np.median([x for x in all_valid_dims if x is not None], axis=0)\n",
        "\n",
        "        group['dimensions'] = group['dimensions'].apply(\n",
        "            lambda x: tuple(median_dims) if x is None or not all(isinstance(v, (int, float)) for v in x) else x\n",
        "        )\n",
        "        return group\n",
        "\n",
        "    df = df.groupby('BRAND').apply(impute_dimensions, include_groups=False).reset_index()\n",
        "\n",
        "    # Compute product_size\n",
        "    df['product_size'] = df['dimensions'].apply(\n",
        "        lambda x: x[0] * x[1] * x[2] if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Compute box_volume (add 10% padding to each dimension)\n",
        "    df['box_volume'] = df['dimensions'].apply(\n",
        "        lambda x: (x[0] * 1.1) * (x[1] * 1.1) * (x[2] * 1.1) if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Filter outliers in product_size (relaxed to 2*IQR)\n",
        "    Q1 = df['product_size'].quantile(0.25)\n",
        "    Q3 = df['product_size'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[df['product_size'].between(Q1 - 2 * IQR, Q3 + 2 * IQR)]\n",
        "    print(f\"After outlier removal: {len(df)} rows\")\n",
        "\n",
        "    # Drop rows with NaN in ENTITY_LENGTH or product_size\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['ENTITY_LENGTH', 'product_size'])\n",
        "    print(f\"Dropped {original_len - len(df)} rows with NaN in ENTITY_LENGTH or product_size. Remaining: {len(df)}\")\n",
        "\n",
        "    if len(df) < 30:\n",
        "        print(f\"Error: Only {len(df)} valid records available. Need at least 30.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Diagnose Model Performance\n",
        "def diagnose_model(train_r2, valid_r2, train_mse, valid_mse):\n",
        "    print(\"\\nModel Performance Diagnosis:\")\n",
        "    print(f\"Training R²: {train_r2:.2f}, Validation R²: {valid_r2:.2f}\")\n",
        "    print(f\"Training MSE: {train_mse:.2f}, Validation MSE: {valid_mse:.2f}\")\n",
        "\n",
        "    if train_r2 > 0.8 and (train_r2 - valid_r2) > 0.1 and valid_mse > 2 * train_mse:\n",
        "        print(\"Overfitting detected: High training performance, but poor validation performance.\")\n",
        "        return \"overfitting\"\n",
        "    elif train_r2 < 0.6 and valid_r2 < 0.6:\n",
        "        print(\"Underfitting detected: Poor performance on both training and validation sets.\")\n",
        "        return \"underfitting\"\n",
        "    elif abs(train_r2 - valid_r2) < 0.1 and train_r2 > 0.7:\n",
        "        print(\"Good performance: Similar and high performance on both sets.\")\n",
        "        return \"good\"\n",
        "    else:\n",
        "        print(\"Moderate performance: Model may need slight tuning.\")\n",
        "        return \"moderate\"\n",
        "\n",
        "# Step 4: Train Ideal Packaging Model with Optional Tuning\n",
        "def train_ideal_packaging_model(X_train, y_train, X_valid, y_valid, diagnose_result):\n",
        "    print(\"\\nTraining ideal packaging model...\")\n",
        "    poly = None\n",
        "\n",
        "    if diagnose_result in [\"overfitting\", \"moderate\"]:\n",
        "        # Use Ridge regression with hyperparameter tuning\n",
        "        print(\"Applying Ridge regression to address overfitting/moderate performance...\")\n",
        "        ridge = Ridge()\n",
        "        param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "        grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        model = grid_search.best_estimator_\n",
        "        print(f\"Best Ridge alpha: {grid_search.best_params_['alpha']}\")\n",
        "\n",
        "    elif diagnose_result == \"underfitting\":\n",
        "        # Add polynomial features and use Ridge regression\n",
        "        print(\"Applying polynomial features and Ridge regression to address underfitting...\")\n",
        "        poly = PolynomialFeatures(degree=2)\n",
        "        X_train_poly = poly.fit_transform(X_train)\n",
        "        X_valid_poly = poly.transform(X_valid)\n",
        "\n",
        "        ridge = Ridge()\n",
        "        param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "        grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "        grid_search.fit(X_train_poly, y_train)\n",
        "        model = grid_search.best_estimator_\n",
        "        print(f\"Best Ridge alpha: {grid_search.best_params_['alpha']}\")\n",
        "\n",
        "        # Update X_train and X_valid for evaluation\n",
        "        X_train = X_train_poly\n",
        "        X_valid = X_valid_poly\n",
        "    else:\n",
        "        # Use standard Linear Regression\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on training set\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    print(f\"Ideal Packaging Model - Train MSE: {train_mse:.2f}, Train R²: {train_r2:.2f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred_valid = model.predict(X_valid)\n",
        "    valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
        "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
        "    print(f\"Ideal Packaging Model - Validation MSE: {valid_mse:.2f}, Validation R²: {valid_r2:.2f}\")\n",
        "\n",
        "    return model, train_mse, train_r2, valid_mse, valid_r2, poly\n",
        "\n",
        "# Step 5: Detect Flaws\n",
        "def detect_flaws(df_valid, ideal_model, poly=None):\n",
        "    X_valid = df_valid[['ENTITY_LENGTH']].values\n",
        "    if poly is not None:\n",
        "        X_valid = poly.transform(X_valid)  # Apply polynomial transformation if used\n",
        "    df_valid['ideal_box_volume'] = ideal_model.predict(X_valid)\n",
        "    df_valid['ideal_product_size'] = df_valid['ideal_box_volume'] * 0.9\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid['design_flaw'] = df_valid['product_size'] > 2 * df_valid['ideal_product_size']\n",
        "    df_valid['packaging_flaw'] = df_valid['box_volume'] > 1.5 * df_valid['ideal_box_volume']\n",
        "\n",
        "    # Excess volume calculation\n",
        "    df_valid['excess_volume'] = df_valid.apply(\n",
        "        lambda row: row['box_volume'] - row['ideal_box_volume'] if row['packaging_flaw'] else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Recommendation\n",
        "    df_valid['recommendation'] = df_valid.apply(\n",
        "        lambda row: (\n",
        "            f\"Reduce box to {row['ideal_box_volume']:.1f} cm³ (save {row['excess_volume']:.1f} cm³)\"\n",
        "            if row['packaging_flaw']\n",
        "            else \"Redesign bottle to reduce volume\" if row['design_flaw']\n",
        "            else \"No flaw detected\"\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df_valid\n",
        "\n",
        "# Step 6: Main Execution\n",
        "def main():\n",
        "    install_dependencies()\n",
        "\n",
        "    file_path = \"/content/amazon_shampoo_products.csv\"\n",
        "    df = preprocess_data(file_path)\n",
        "\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Split data\n",
        "    df_train, df_valid = train_test_split(df, train_size=0.8, test_size=0.2, random_state=42)\n",
        "    print(f\"Training set size: {len(df_train)} records, Validation set size: {len(df_valid)} records\")\n",
        "\n",
        "    # Prepare data for initial model\n",
        "    X_train = df_train[['ENTITY_LENGTH']].values\n",
        "    y_train = df_train['box_volume']\n",
        "    X_valid = df_valid[['ENTITY_LENGTH']].values\n",
        "    y_valid = df_valid['box_volume']\n",
        "\n",
        "    # Train initial model for diagnostics\n",
        "    initial_model = LinearRegression()\n",
        "    initial_model.fit(X_train, y_train)\n",
        "    y_pred_train = initial_model.predict(X_train)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    y_pred_valid = initial_model.predict(X_valid)\n",
        "    valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
        "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
        "\n",
        "    # Diagnose model\n",
        "    diagnose_result = diagnose_model(train_r2, valid_r2, train_mse, valid_mse)\n",
        "\n",
        "    # Train final model with tuning if needed\n",
        "    model, train_mse, train_r2, valid_mse, valid_r2, poly = train_ideal_packaging_model(X_train, y_train, X_valid, y_valid, diagnose_result)\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid = detect_flaws(df_valid, model, poly)\n",
        "\n",
        "    # Save results\n",
        "    output_df = df_valid[[\n",
        "        'ENTITY_ID', 'ENTITY_NAME', 'ENTITY_LENGTH', 'product_size', 'box_volume',\n",
        "        'ideal_product_size', 'ideal_box_volume', 'design_flaw', 'packaging_flaw',\n",
        "        'excess_volume', 'recommendation'\n",
        "    ]]\n",
        "    output_file = \"packaging_flaw_analysis.csv\"\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nResults saved to '{output_file}'.\")\n",
        "    files.download(output_file)\n",
        "\n",
        "    # Summary\n",
        "    design_flaws = df_valid['design_flaw'].sum()\n",
        "    packaging_flaws = df_valid['packaging_flaw'].sum()\n",
        "    print(f\"Detected {design_flaws} products with design flaws.\")\n",
        "    print(f\"Detected {packaging_flaws} products with packaging flaws.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_6lqElHAMl16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "8c0bf7aa-8ae7-49c6-8936-e703218bfe82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed.\n",
            "Loading data...\n",
            "After removing duplicates: 140 rows\n",
            "After outlier removal: 113 rows\n",
            "Dropped 6 rows with NaN in ENTITY_LENGTH or product_size. Remaining: 107\n",
            "Training set size: 85 records, Validation set size: 22 records\n",
            "\n",
            "Model Performance Diagnosis:\n",
            "Training R²: 0.06, Validation R²: 0.24\n",
            "Training MSE: 881170.88, Validation MSE: 301726.62\n",
            "Underfitting detected: Poor performance on both training and validation sets.\n",
            "\n",
            "Training ideal packaging model...\n",
            "Applying polynomial features and Ridge regression to address underfitting...\n",
            "Best Ridge alpha: 100\n",
            "Ideal Packaging Model - Train MSE: 858774.51, Train R²: 0.09\n",
            "Ideal Packaging Model - Validation MSE: 351669.98, Validation R²: 0.11\n",
            "\n",
            "Results saved to 'packaging_flaw_analysis.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a0f78081-3ad4-44fa-83c3-71e075051e85\", \"packaging_flaw_analysis.csv\", 6747)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 0 products with design flaws.\n",
            "Detected 2 products with packaging flaws.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Step 1: Install Dependencies\n",
        "def install_dependencies():\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q pandas numpy scikit-learn\")\n",
        "    print(\"Dependencies installed.\")\n",
        "\n",
        "# Step 2: Load and Preprocess Data\n",
        "def preprocess_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['ENTITY_NAME', 'ENTITY_LENGTH', 'DIMENSIONS', 'PRICE', 'BRAND'])\n",
        "    print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "    # Parse DIMENSIONS safely\n",
        "    df['dimensions'] = df['DIMENSIONS'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Impute missing DIMENSIONS and ENTITY_LENGTH with brand-specific medians\n",
        "    def impute_missing(group):\n",
        "        # Impute dimensions\n",
        "        valid_dims = group['dimensions'].apply(\n",
        "            lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "        )\n",
        "        valid_dims_list = [x for x in valid_dims if x is not None]\n",
        "        if valid_dims_list:\n",
        "            median_dims = np.median(valid_dims_list, axis=0)\n",
        "        else:\n",
        "            all_valid_dims = df['dimensions'].apply(\n",
        "                lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "            )\n",
        "            median_dims = np.median([x for x in all_valid_dims if x is not None], axis=0)\n",
        "\n",
        "        group['dimensions'] = group['dimensions'].apply(\n",
        "            lambda x: tuple(median_dims) if x is None or not (isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x)) else x\n",
        "        )\n",
        "\n",
        "        # Impute ENTITY_LENGTH\n",
        "        valid_lengths = group['ENTITY_LENGTH'].apply(\n",
        "            lambda x: x if isinstance(x, (int, float)) and not np.isnan(x) else None\n",
        "        )\n",
        "        valid_lengths_list = [x for x in valid_lengths if x is not None]\n",
        "        median_length = np.median(valid_lengths_list) if valid_lengths_list else df['ENTITY_LENGTH'].median()\n",
        "\n",
        "        group['ENTITY_LENGTH'] = group['ENTITY_LENGTH'].fillna(median_length)\n",
        "        return group\n",
        "\n",
        "    df = df.groupby('BRAND').apply(impute_missing, include_groups=False).reset_index()\n",
        "\n",
        "    # Extract individual dimensions\n",
        "    df['dim_length'] = df['dimensions'].apply(lambda x: x[0])\n",
        "    df['dim_width'] = df['dimensions'].apply(lambda x: x[1])\n",
        "    df['dim_height'] = df['dimensions'].apply(lambda x: x[2])\n",
        "\n",
        "    # Compute product_size\n",
        "    df['product_size'] = df['dimensions'].apply(\n",
        "        lambda x: x[0] * x[1] * x[2] if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Compute box_volume (add 10% padding to each dimension)\n",
        "    df['box_volume'] = df['dimensions'].apply(\n",
        "        lambda x: (x[0] * 1.1) * (x[1] * 1.1) * (x[2] * 1.1) if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Filter outliers in product_size (2*IQR)\n",
        "    Q1 = df['product_size'].quantile(0.25)\n",
        "    Q3 = df['product_size'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[df['product_size'].between(Q1 - 2 * IQR, Q3 + 2 * IQR)]\n",
        "    print(f\"After outlier removal: {len(df)} rows\")\n",
        "\n",
        "    # Drop rows with NaN in key features\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['ENTITY_LENGTH', 'PRICE', 'dim_length', 'dim_width', 'dim_height', 'product_size'])\n",
        "    print(f\"Dropped {original_len - len(df)} rows with NaN in key features. Remaining: {len(df)}\")\n",
        "\n",
        "    if len(df) < 30:\n",
        "        print(f\"Error: Only {len(df)} valid records available. Need at least 30.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Diagnose Model Performance\n",
        "def diagnose_model(train_r2, valid_r2, train_mse, valid_mse):\n",
        "    print(\"\\nModel Performance Diagnosis:\")\n",
        "    print(f\"Training R²: {train_r2:.2f}, Validation R²: {valid_r2:.2f}\")\n",
        "    print(f\"Training MSE: {train_mse:.2f}, Validation MSE: {valid_mse:.2f}\")\n",
        "\n",
        "    if train_r2 > 0.8 and (train_r2 - valid_r2) > 0.1 and valid_mse > 2 * train_mse:\n",
        "        print(\"Overfitting detected: High training performance, but poor validation performance.\")\n",
        "        return \"overfitting\"\n",
        "    elif train_r2 < 0.6 and valid_r2 < 0.6:\n",
        "        print(\"Underfitting detected: Poor performance on both training and validation sets.\")\n",
        "        return \"underfitting\"\n",
        "    elif abs(train_r2 - valid_r2) < 0.1 and train_r2 > 0.7:\n",
        "        print(\"Good performance: Similar and high performance on both sets.\")\n",
        "        return \"good\"\n",
        "    else:\n",
        "        print(\"Moderate performance: Model may need slight tuning.\")\n",
        "        return \"moderate\"\n",
        "\n",
        "# Step 4: Train Ideal Packaging Model\n",
        "def train_ideal_packaging_model(X_train, y_train, X_valid, y_valid):\n",
        "    print(\"\\nTraining ideal packaging model with Random Forest...\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "    # Define Random Forest model with hyperparameter tuning\n",
        "    rf = RandomForestRegressor(random_state=42)\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [5, 10, None],\n",
        "        'min_samples_split': [2, 5]\n",
        "    }\n",
        "    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    model = grid_search.best_estimator_\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "    # Evaluate on training set\n",
        "    y_pred_train = model.predict(X_train_scaled)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    print(f\"Ideal Packaging Model - Train MSE: {train_mse:.4f}, Train R²: {train_r2:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred_valid = model.predict(X_valid_scaled)\n",
        "    valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
        "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
        "    print(f\"Ideal Packaging Model - Validation MSE: {valid_mse:.4f}, Validation R²: {valid_r2:.4f}\")\n",
        "\n",
        "    return model, scaler, train_mse, train_r2, valid_mse, valid_r2\n",
        "\n",
        "# Step 5: Detect Flaws\n",
        "def detect_flaws(df_valid, model, scaler):\n",
        "    X_valid = df_valid[['ENTITY_LENGTH', 'PRICE', 'dim_length', 'dim_width', 'dim_height']].values\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    df_valid['ideal_box_volume'] = model.predict(X_valid_scaled)\n",
        "    df_valid['ideal_product_size'] = df_valid['ideal_box_volume'] * 0.9\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid['design_flaw'] = df_valid['product_size'] > 2 * df_valid['ideal_product_size']\n",
        "    df_valid['packaging_flaw'] = df_valid['box_volume'] > 1.5 * df_valid['ideal_box_volume']\n",
        "\n",
        "    # Excess volume calculation\n",
        "    df_valid['excess_volume'] = df_valid.apply(\n",
        "        lambda row: row['box_volume'] - row['ideal_box_volume'] if row['packaging_flaw'] else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Recommendation\n",
        "    df_valid['recommendation'] = df_valid.apply(\n",
        "        lambda row: (\n",
        "            f\"Reduce box to {row['ideal_box_volume']:.1f} cm³ (save {row['excess_volume']:.1f} cm³)\"\n",
        "            if row['packaging_flaw']\n",
        "            else \"Redesign bottle to reduce volume\" if row['design_flaw']\n",
        "            else \"No flaw detected\"\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df_valid\n",
        "\n",
        "# Step 6: Main Execution\n",
        "def main():\n",
        "    install_dependencies()\n",
        "\n",
        "    file_path = \"amazon_shampoo_products.csv\"\n",
        "    df = preprocess_data(file_path)\n",
        "\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Split data\n",
        "    df_train, df_valid = train_test_split(df, train_size=0.8, test_size=0.2, random_state=42)\n",
        "    print(f\"Training set size: {len(df_train)} records, Validation set size: {len(df_valid)} records\")\n",
        "\n",
        "    # Prepare data for model\n",
        "    features = ['ENTITY_LENGTH', 'PRICE', 'dim_length', 'dim_width', 'dim_height']\n",
        "    X_train = df_train[features].values\n",
        "    y_train = df_train['box_volume']\n",
        "    X_valid = df_valid[features].values\n",
        "    y_valid = df_valid['box_volume']\n",
        "\n",
        "    # Train model\n",
        "    model, scaler, train_mse, train_r2, valid_mse, valid_r2 = train_ideal_packaging_model(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    # Diagnose model\n",
        "    diagnose_result = diagnose_model(train_r2, valid_r2, train_mse, valid_mse)\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid = detect_flaws(df_valid, model, scaler)\n",
        "\n",
        "    # Save results\n",
        "    output_df = df_valid[[\n",
        "        'ENTITY_ID', 'ENTITY_NAME', 'ENTITY_LENGTH', 'product_size', 'box_volume',\n",
        "        'ideal_product_size', 'ideal_box_volume', 'design_flaw', 'packaging_flaw',\n",
        "        'excess_volume', 'recommendation'\n",
        "    ]]\n",
        "    output_file = \"packaging_flaw_analysis.csv\"\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nResults saved to '{output_file}'.\")\n",
        "\n",
        "    # Summary\n",
        "    design_flaws = df_valid['design_flaw'].sum()\n",
        "    packaging_flaws = df_valid['packaging_flaw'].sum()\n",
        "    print(f\"Detected {design_flaws} products with design flaws.\")\n",
        "    print(f\"Detected {packaging_flaws} products with packaging flaws.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDLmlnqel4uD",
        "outputId": "73dad98a-816f-4903-b80a-31a11fe5796c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed.\n",
            "Loading data...\n",
            "After removing duplicates: 140 rows\n",
            "After outlier removal: 113 rows\n",
            "Dropped 2 rows with NaN in key features. Remaining: 111\n",
            "Training set size: 88 records, Validation set size: 23 records\n",
            "\n",
            "Training ideal packaging model with Random Forest...\n",
            "Best parameters: {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Ideal Packaging Model - Train MSE: 33509.4305, Train R²: 0.9661\n",
            "Ideal Packaging Model - Validation MSE: 238019.7336, Validation R²: 0.3518\n",
            "\n",
            "Model Performance Diagnosis:\n",
            "Training R²: 0.97, Validation R²: 0.35\n",
            "Training MSE: 33509.43, Validation MSE: 238019.73\n",
            "Overfitting detected: High training performance, but poor validation performance.\n",
            "\n",
            "Results saved to 'packaging_flaw_analysis.csv'.\n",
            "Detected 0 products with design flaws.\n",
            "Detected 1 products with packaging flaws.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Step 1: Install Dependencies\n",
        "def install_dependencies():\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q pandas numpy scikit-learn\")\n",
        "    print(\"Dependencies installed.\")\n",
        "\n",
        "# Step 2: Load and Preprocess Data\n",
        "def preprocess_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['ENTITY_NAME', 'ENTITY_LENGTH', 'DIMENSIONS', 'PRICE', 'BRAND'])\n",
        "    print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "    # Parse DIMENSIONS safely\n",
        "    df['dimensions'] = df['DIMENSIONS'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Impute missing DIMENSIONS and ENTITY_LENGTH with brand-specific medians\n",
        "    def impute_missing(group):\n",
        "        # Impute dimensions\n",
        "        valid_dims = group['dimensions'].apply(\n",
        "            lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "        )\n",
        "        valid_dims_list = [x for x in valid_dims if x is not None]\n",
        "        if valid_dims_list:\n",
        "            median_dims = np.median(valid_dims_list, axis=0)\n",
        "        else:\n",
        "            all_valid_dims = df['dimensions'].apply(\n",
        "                lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "            )\n",
        "            median_dims = np.median([x for x in all_valid_dims if x is not None], axis=0)\n",
        "\n",
        "        group['dimensions'] = group['dimensions'].apply(\n",
        "            lambda x: tuple(median_dims) if x is None or not (isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x)) else x\n",
        "        )\n",
        "\n",
        "        # Impute ENTITY_LENGTH\n",
        "        valid_lengths = group['ENTITY_LENGTH'].apply(\n",
        "            lambda x: x if isinstance(x, (int, float)) and not np.isnan(x) else None\n",
        "        )\n",
        "        valid_lengths_list = [x for x in valid_lengths if x is not None]\n",
        "        median_length = np.median(valid_lengths_list) if valid_lengths_list else df['ENTITY_LENGTH'].median()\n",
        "\n",
        "        group['ENTITY_LENGTH'] = group['ENTITY_LENGTH'].fillna(median_length)\n",
        "        return group\n",
        "\n",
        "    df = df.groupby('BRAND').apply(impute_missing, include_groups=False).reset_index()\n",
        "\n",
        "    # Extract individual dimensions\n",
        "    df['dim_length'] = df['dimensions'].apply(lambda x: x[0])\n",
        "    df['dim_width'] = df['dimensions'].apply(lambda x: x[1])\n",
        "    df['dim_height'] = df['dimensions'].apply(lambda x: x[2])\n",
        "\n",
        "    # Compute product_size\n",
        "    df['product_size'] = df['dimensions'].apply(\n",
        "        lambda x: x[0] * x[1] * x[2] if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Compute box_volume (add 10% padding to each dimension)\n",
        "    df['box_volume'] = df['dimensions'].apply(\n",
        "        lambda x: (x[0] * 1.1) * (x[1] * 1.1) * (x[2] * 1.1) if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Filter outliers in product_size (2*IQR)\n",
        "    Q1 = df['product_size'].quantile(0.25)\n",
        "    Q3 = df['product_size'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[df['product_size'].between(Q1 - 2 * IQR, Q3 + 2 * IQR)]\n",
        "    print(f\"After outlier removal: {len(df)} rows\")\n",
        "\n",
        "    # Drop rows with NaN in key features\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['ENTITY_LENGTH', 'PRICE', 'product_size'])\n",
        "    print(f\"Dropped {original_len - len(df)} rows with NaN in key features. Remaining: {len(df)}\")\n",
        "\n",
        "    if len(df) < 30:\n",
        "        print(f\"Error: Only {len(df)} valid records available. Need at least 30.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Diagnose Model Performance\n",
        "def diagnose_model(train_r2, valid_r2, train_mse, valid_mse):\n",
        "    print(\"\\nModel Performance Diagnosis:\")\n",
        "    print(f\"Training R²: {train_r2:.2f}, Validation R²: {valid_r2:.2f}\")\n",
        "    print(f\"Training MSE: {train_mse:.2f}, Validation MSE: {valid_mse:.2f}\")\n",
        "\n",
        "    if train_r2 > 0.8 and (train_r2 - valid_r2) > 0.1 and valid_mse > 2 * train_mse:\n",
        "        print(\"Overfitting detected: High training performance, but poor validation performance.\")\n",
        "        return \"overfitting\"\n",
        "    elif train_r2 < 0.6 and valid_r2 < 0.6:\n",
        "        print(\"Underfitting detected: Poor performance on both training and validation sets.\")\n",
        "        return \"underfitting\"\n",
        "    elif abs(train_r2 - valid_r2) < 0.1 and train_r2 > 0.7:\n",
        "        print(\"Good performance: Similar and high performance on both sets.\")\n",
        "        return \"good\"\n",
        "    else:\n",
        "        print(\"Moderate performance: Model may need slight tuning.\")\n",
        "        return \"moderate\"\n",
        "\n",
        "# Step 4: Train Ideal Packaging Model\n",
        "def train_ideal_packaging_model(X_train, y_train, X_valid, y_valid):\n",
        "    print(\"\\nTraining ideal packaging model with Random Forest...\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "    # Define Random Forest model with hyperparameter tuning\n",
        "    rf = RandomForestRegressor(random_state=42)\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [3, 5],\n",
        "        'min_samples_split': [5, 10],\n",
        "        'min_samples_leaf': [2, 4]\n",
        "    }\n",
        "    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    model = grid_search.best_estimator_\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "    # Evaluate on training set\n",
        "    y_pred_train = model.predict(X_train_scaled)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    print(f\"Ideal Packaging Model - Train MSE: {train_mse:.4f}, Train R²: {train_r2:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred_valid = model.predict(X_valid_scaled)\n",
        "    valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
        "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
        "    print(f\"Ideal Packaging Model - Validation MSE: {valid_mse:.4f}, Validation R²: {valid_r2:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    feature_names = ['ENTITY_LENGTH', 'PRICE']\n",
        "    importances = model.feature_importances_\n",
        "    for name, importance in zip(feature_names, importances):\n",
        "        print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
        "\n",
        "    return model, scaler, train_mse, train_r2, valid_mse, valid_r2\n",
        "\n",
        "# Step 5: Detect Flaws\n",
        "def detect_flaws(df_valid, model, scaler):\n",
        "    X_valid = df_valid[['ENTITY_LENGTH', 'PRICE']].values\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    df_valid['ideal_box_volume'] = model.predict(X_valid_scaled)\n",
        "    df_valid['ideal_product_size'] = df_valid['ideal_box_volume'] * 0.9\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid['design_flaw'] = df_valid['product_size'] > 2 * df_valid['ideal_product_size']\n",
        "    df_valid['packaging_flaw'] = df_valid['box_volume'] > 1.5 * df_valid['ideal_box_volume']\n",
        "\n",
        "    # Excess volume calculation\n",
        "    df_valid['excess_volume'] = df_valid.apply(\n",
        "        lambda row: row['box_volume'] - row['ideal_box_volume'] if row['packaging_flaw'] else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Recommendation\n",
        "    df_valid['recommendation'] = df_valid.apply(\n",
        "        lambda row: (\n",
        "            f\"Reduce box to {row['ideal_box_volume']:.1f} cm³ (save {row['excess_volume']:.1f} cm³)\"\n",
        "            if row['packaging_flaw']\n",
        "            else \"Redesign bottle to reduce volume\" if row['design_flaw']\n",
        "            else \"No flaw detected\"\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df_valid\n",
        "\n",
        "# Step 6: Main Execution\n",
        "def main():\n",
        "    install_dependencies()\n",
        "\n",
        "    file_path = \"amazon_shampoo_products.csv\"\n",
        "    df = preprocess_data(file_path)\n",
        "\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Split data\n",
        "    df_train, df_valid = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
        "    print(f\"Training set size: {len(df_train)} records, Validation set size: {len(df_valid)} records\")\n",
        "\n",
        "    # Prepare data for model\n",
        "    features = ['ENTITY_LENGTH', 'PRICE']\n",
        "    X_train = df_train[features].values\n",
        "    y_train = df_train['box_volume']\n",
        "    X_valid = df_valid[features].values\n",
        "    y_valid = df_valid['box_volume']\n",
        "\n",
        "    # Train model\n",
        "    model, scaler, train_mse, train_r2, valid_mse, valid_r2 = train_ideal_packaging_model(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    # Diagnose model\n",
        "    diagnose_result = diagnose_model(train_r2, valid_r2, train_mse, valid_mse)\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid = detect_flaws(df_valid, model, scaler)\n",
        "\n",
        "    # Save results\n",
        "    output_df = df_valid[[\n",
        "        'ENTITY_ID', 'ENTITY_NAME', 'ENTITY_LENGTH', 'product_size', 'box_volume',\n",
        "        'ideal_product_size', 'ideal_box_volume', 'design_flaw', 'packaging_flaw',\n",
        "        'excess_volume', 'recommendation'\n",
        "    ]]\n",
        "    output_file = \"packaging_output.csv\"\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nResults saved to '{output_file}'.\")\n",
        "\n",
        "    # Summary\n",
        "    design_flaws = df_valid['design_flaw'].sum()\n",
        "    packaging_flaws = df_valid['packaging_flaw'].sum()\n",
        "    print(f\"Detected {design_flaws} products with design flaws.\")\n",
        "    print(f\"Detected {packaging_flaws} products with packaging flaws.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBsD_03gspiv",
        "outputId": "5e1f0f1b-9337-4c40-a062-22dab99f04e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed.\n",
            "Loading data...\n",
            "After removing duplicates: 140 rows\n",
            "After outlier removal: 113 rows\n",
            "Dropped 2 rows with NaN in key features. Remaining: 111\n",
            "Training set size: 77 records, Validation set size: 34 records\n",
            "\n",
            "Training ideal packaging model with Random Forest...\n",
            "Best parameters: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "Ideal Packaging Model - Train MSE: 709824.5782, Train R²: 0.3219\n",
            "Ideal Packaging Model - Validation MSE: 448977.2486, Validation R²: -0.0025\n",
            "Feature: ENTITY_LENGTH, Importance: 0.2550\n",
            "Feature: PRICE, Importance: 0.7450\n",
            "\n",
            "Model Performance Diagnosis:\n",
            "Training R²: 0.32, Validation R²: -0.00\n",
            "Training MSE: 709824.58, Validation MSE: 448977.25\n",
            "Underfitting detected: Poor performance on both training and validation sets.\n",
            "\n",
            "Results saved to 'packaging_output.csv'.\n",
            "Detected 0 products with design flaws.\n",
            "Detected 2 products with packaging flaws.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Step 1: Install Dependencies\n",
        "def install_dependencies():\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q pandas numpy scikit-learn xgboost\")\n",
        "    print(\"Dependencies installed.\")\n",
        "\n",
        "# Step 2: Load and Preprocess Data\n",
        "def preprocess_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['ENTITY_NAME', 'ENTITY_LENGTH', 'DIMENSIONS', 'PRICE', 'BRAND'])\n",
        "    print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "    # Parse DIMENSIONS safely\n",
        "    df['dimensions'] = df['DIMENSIONS'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Impute missing DIMENSIONS and ENTITY_LENGTH with brand-specific medians\n",
        "    def impute_missing(group):\n",
        "        # Impute dimensions\n",
        "        valid_dims = group['dimensions'].apply(\n",
        "            lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "        )\n",
        "        valid_dims_list = [x for x in valid_dims if x is not None]\n",
        "        if valid_dims_list:\n",
        "            median_dims = np.median(valid_dims_list, axis=0)\n",
        "        else:\n",
        "            all_valid_dims = df['dimensions'].apply(\n",
        "                lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "            )\n",
        "            median_dims = np.median([x for x in all_valid_dims if x is not None], axis=0)\n",
        "\n",
        "        group['dimensions'] = group['dimensions'].apply(\n",
        "            lambda x: tuple(median_dims) if x is None or not (isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x)) else x\n",
        "        )\n",
        "\n",
        "        # Impute ENTITY_LENGTH\n",
        "        valid_lengths = group['ENTITY_LENGTH'].apply(\n",
        "            lambda x: x if isinstance(x, (int, float)) and not np.isnan(x) else None\n",
        "        )\n",
        "        valid_lengths_list = [x for x in valid_lengths if x is not None]\n",
        "        median_length = np.median(valid_lengths_list) if valid_lengths_list else df['ENTITY_LENGTH'].median()\n",
        "\n",
        "        group['ENTITY_LENGTH'] = group['ENTITY_LENGTH'].fillna(median_length)\n",
        "        return group\n",
        "\n",
        "    df = df.groupby('BRAND').apply(impute_missing, include_groups=False).reset_index()\n",
        "\n",
        "    # Extract individual dimensions\n",
        "    df['dim_length'] = df['dimensions'].apply(lambda x: x[0])\n",
        "    df['dim_width'] = df['dimensions'].apply(lambda x: x[1])\n",
        "    df['dim_height'] = df['dimensions'].apply(lambda x: x[2])\n",
        "\n",
        "    # Compute product_size\n",
        "    df['product_size'] = df['dimensions'].apply(\n",
        "        lambda x: x[0] * x[1] * x[2] if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Compute box_volume (add 10% padding to each dimension)\n",
        "    df['box_volume'] = df['dimensions'].apply(\n",
        "        lambda x: (x[0] * 1.1) * (x[1] * 1.1) * (x[2] * 1.1) if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Filter outliers in product_size (2*IQR)\n",
        "    Q1 = df['product_size'].quantile(0.25)\n",
        "    Q3 = df['product_size'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[df['product_size'].between(Q1 - 2 * IQR, Q3 + 2 * IQR)]\n",
        "    print(f\"After outlier removal: {len(df)} rows\")\n",
        "\n",
        "    # Drop rows with NaN in key features\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['ENTITY_LENGTH', 'PRICE', 'dim_length', 'dim_width', 'dim_height', 'product_size'])\n",
        "    print(f\"Dropped {original_len - len(df)} rows with NaN in key features. Remaining: {len(df)}\")\n",
        "\n",
        "    if len(df) < 30:\n",
        "        print(f\"Error: Only {len(df)} valid records available. Need at least 30.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Diagnose Model Performance\n",
        "def diagnose_model(train_r2, valid_r2, train_mse, valid_mse):\n",
        "    print(\"\\nModel Performance Diagnosis:\")\n",
        "    print(f\"Training R²: {train_r2:.2f}, Validation R²: {valid_r2:.2f}\")\n",
        "    print(f\"Training MSE: {train_mse:.2f}, Validation MSE: {valid_mse:.2f}\")\n",
        "\n",
        "    if train_r2 > 0.8 and (train_r2 - valid_r2) > 0.1 and valid_mse > 2 * train_mse:\n",
        "        print(\"Overfitting detected: High training performance, but poor validation performance.\")\n",
        "        return \"overfitting\"\n",
        "    elif train_r2 < 0.6 and valid_r2 < 0.6:\n",
        "        print(\"Underfitting detected: Poor performance on both training and validation sets.\")\n",
        "        return \"underfitting\"\n",
        "    elif abs(train_r2 - valid_r2) < 0.1 and train_r2 > 0.7:\n",
        "        print(\"Good performance: Similar and high performance on both sets.\")\n",
        "        return \"good\"\n",
        "    else:\n",
        "        print(\"Moderate performance: Model may need slight tuning.\")\n",
        "        return \"moderate\"\n",
        "\n",
        "# Step 4: Train Ideal Packaging Model\n",
        "def train_ideal_packaging_model(X_train, y_train, X_valid, y_valid):\n",
        "    print(\"\\nTraining ideal packaging model with XGBoost...\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "    # Define XGBoost model with hyperparameter tuning\n",
        "    xgb = XGBRegressor(random_state=42)\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [3, 5],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'subsample': [0.8, 1.0]\n",
        "    }\n",
        "    grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    model = grid_search.best_estimator_\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "    # Evaluate on training set\n",
        "    y_pred_train = model.predict(X_train_scaled)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    print(f\"Ideal Packaging Model - Train MSE: {train_mse:.4f}, Train R²: {train_r2:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred_valid = model.predict(X_valid_scaled)\n",
        "    valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
        "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
        "    print(f\"Ideal Packaging Model - Validation MSE: {valid_mse:.4f}, Validation R²: {valid_r2:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    feature_names = ['ENTITY_LENGTH', 'PRICE', 'dim_length', 'dim_width', 'dim_height']\n",
        "    importances = model.feature_importances_\n",
        "    for name, importance in zip(feature_names, importances):\n",
        "        print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
        "\n",
        "    return model, scaler, train_mse, train_r2, valid_mse, valid_r2\n",
        "\n",
        "# Step 5: Detect Flaws\n",
        "def detect_flaws(df_valid, model, scaler):\n",
        "    X_valid = df_valid[['ENTITY_LENGTH', 'PRICE', 'dim_length', 'dim_width', 'dim_height']].values\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    df_valid['ideal_box_volume'] = model.predict(X_valid_scaled)\n",
        "    df_valid['ideal_product_size'] = df_valid['ideal_box_volume'] * 0.9\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid['design_flaw'] = df_valid['product_size'] > 2 * df_valid['ideal_product_size']\n",
        "    df_valid['packaging_flaw'] = df_valid['box_volume'] > 1.5 * df_valid['ideal_box_volume']\n",
        "\n",
        "    # Excess volume calculation\n",
        "    df_valid['excess_volume'] = df_valid.apply(\n",
        "        lambda row: row['box_volume'] - row['ideal_box_volume'] if row['packaging_flaw'] else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Recommendation\n",
        "    df_valid['recommendation'] = df_valid.apply(\n",
        "        lambda row: (\n",
        "            f\"Reduce box to {row['ideal_box_volume']:.1f} cm³ (save {row['excess_volume']:.1f} cm³)\"\n",
        "            if row['packaging_flaw']\n",
        "            else \"Redesign bottle to reduce volume\" if row['design_flaw']\n",
        "            else \"No flaw detected\"\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Outlier analysis\n",
        "    print(\"\\nValidation Set Outlier Analysis:\")\n",
        "    Q1 = df_valid['box_volume'].quantile(0.25)\n",
        "    Q3 = df_valid['box_volume'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df_valid[df_valid['box_volume'] > Q3 + 1.5 * IQR]\n",
        "    if not outliers.empty:\n",
        "        print(f\"Found {len(outliers)} potential outliers in validation set 'box_volume':\")\n",
        "        print(outliers[['ENTITY_NAME', 'box_volume', 'product_size']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No outliers detected in validation set 'box_volume'.\")\n",
        "\n",
        "    return df_valid\n",
        "\n",
        "# Step 6: Main Execution\n",
        "def main():\n",
        "    install_dependencies()\n",
        "\n",
        "    file_path = \"amazon_shampoo_products.csv\"\n",
        "    df = preprocess_data(file_path)\n",
        "\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Split data\n",
        "    df_train, df_valid = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
        "    print(f\"Training set size: {len(df_train)} records, Validation set size: {len(df_valid)} records\")\n",
        "\n",
        "    # Prepare data for model\n",
        "    features = ['ENTITY_LENGTH', 'PRICE', 'dim_length', 'dim_width', 'dim_height']\n",
        "    X_train = df_train[features].values\n",
        "    y_train = df_train['box_volume']\n",
        "    X_valid = df_valid[features].values\n",
        "    y_valid = df_valid['box_volume']\n",
        "\n",
        "    # Train model\n",
        "    model, scaler, train_mse, train_r2, valid_mse, valid_r2 = train_ideal_packaging_model(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    # Diagnose model\n",
        "    diagnose_result = diagnose_model(train_r2, valid_r2, train_mse, valid_mse)\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid = detect_flaws(df_valid, model, scaler)\n",
        "\n",
        "    # Save results\n",
        "    output_df = df_valid[[\n",
        "        'ENTITY_ID', 'ENTITY_NAME', 'ENTITY_LENGTH', 'product_size', 'box_volume',\n",
        "        'ideal_product_size', 'ideal_box_volume', 'design_flaw', 'packaging_flaw',\n",
        "        'excess_volume', 'recommendation'\n",
        "    ]]\n",
        "    output_file = \"packaging_output_v3.csv\"\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nResults saved to '{output_file}'.\")\n",
        "\n",
        "    # Summary\n",
        "    design_flaws = df_valid['design_flaw'].sum()\n",
        "    packaging_flaws = df_valid['packaging_flaw'].sum()\n",
        "    print(f\"Detected {design_flaws} products with design flaws.\")\n",
        "    print(f\"Detected {packaging_flaws} products with packaging flaws.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FagyiUszt-pi",
        "outputId": "da4b349b-b586-4bd8-a48d-4b3d72a6fa15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed.\n",
            "Loading data...\n",
            "After removing duplicates: 140 rows\n",
            "After outlier removal: 113 rows\n",
            "Dropped 2 rows with NaN in key features. Remaining: 111\n",
            "Training set size: 77 records, Validation set size: 34 records\n",
            "\n",
            "Training ideal packaging model with XGBoost...\n",
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Ideal Packaging Model - Train MSE: 633.6145, Train R²: 0.9994\n",
            "Ideal Packaging Model - Validation MSE: 53845.8873, Validation R²: 0.8798\n",
            "Feature: ENTITY_LENGTH, Importance: 0.0351\n",
            "Feature: PRICE, Importance: 0.0342\n",
            "Feature: dim_length, Importance: 0.1222\n",
            "Feature: dim_width, Importance: 0.7074\n",
            "Feature: dim_height, Importance: 0.1012\n",
            "\n",
            "Model Performance Diagnosis:\n",
            "Training R²: 1.00, Validation R²: 0.88\n",
            "Training MSE: 633.61, Validation MSE: 53845.89\n",
            "Overfitting detected: High training performance, but poor validation performance.\n",
            "\n",
            "Validation Set Outlier Analysis:\n",
            "No outliers detected in validation set 'box_volume'.\n",
            "\n",
            "Results saved to 'packaging_output_v3.csv'.\n",
            "Detected 2 products with design flaws.\n",
            "Detected 2 products with packaging flaws.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Step 1: Install Dependencies\n",
        "def install_dependencies():\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q pandas numpy scikit-learn xgboost\")\n",
        "    print(\"Dependencies installed.\")\n",
        "\n",
        "# Step 2: Load and Preprocess Data\n",
        "def preprocess_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['ENTITY_NAME', 'ENTITY_LENGTH', 'DIMENSIONS', 'PRICE', 'BRAND'])\n",
        "    print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "    # Parse DIMENSIONS safely\n",
        "    df['dimensions'] = df['DIMENSIONS'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Impute missing DIMENSIONS with brand-specific medians\n",
        "    def impute_missing(group):\n",
        "        valid_dims = group['dimensions'].apply(\n",
        "            lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "        )\n",
        "        valid_dims_list = [x for x in valid_dims if x is not None]\n",
        "        if valid_dims_list:\n",
        "            median_dims = np.median(valid_dims_list, axis=0)\n",
        "        else:\n",
        "            all_valid_dims = df['dimensions'].apply(\n",
        "                lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "            )\n",
        "            median_dims = np.median([x for x in all_valid_dims if x is not None], axis=0)\n",
        "\n",
        "        group['dimensions'] = group['dimensions'].apply(\n",
        "            lambda x: tuple(median_dims) if x is None or not (isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x)) else x\n",
        "        )\n",
        "        return group\n",
        "\n",
        "    df = df.groupby('BRAND').apply(impute_missing, include_groups=False).reset_index()\n",
        "\n",
        "    # Extract individual dimensions\n",
        "    df['dim_length'] = df['dimensions'].apply(lambda x: x[0])\n",
        "    df['dim_width'] = df['dimensions'].apply(lambda x: x[1])\n",
        "    df['dim_height'] = df['dimensions'].apply(lambda x: x[2])\n",
        "\n",
        "    # Compute product_size\n",
        "    df['product_size'] = df['dimensions'].apply(\n",
        "        lambda x: x[0] * x[1] * x[2] if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Compute box_volume (add 10% padding to each dimension)\n",
        "    df['box_volume'] = df['dimensions'].apply(\n",
        "        lambda x: (x[0] * 1.1) * (x[1] * 1.1) * (x[2] * 1.1) if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Filter outliers in product_size (2*IQR)\n",
        "    Q1 = df['product_size'].quantile(0.25)\n",
        "    Q3 = df['product_size'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[df['product_size'].between(Q1 - 2 * IQR, Q3 + 2 * IQR)]\n",
        "    print(f\"After outlier removal: {len(df)} rows\")\n",
        "\n",
        "    # Drop rows with NaN in key features\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['dim_length', 'dim_width', 'dim_height', 'product_size'])\n",
        "    print(f\"Dropped {original_len - len(df)} rows with NaN in key features. Remaining: {len(df)}\")\n",
        "\n",
        "    if len(df) < 30:\n",
        "        print(f\"Error: Only {len(df)} valid records available. Need at least 30.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Diagnose Model Performance\n",
        "def diagnose_model(train_r2, valid_r2, train_mse, valid_mse):\n",
        "    print(\"\\nModel Performance Diagnosis:\")\n",
        "    print(f\"Training R²: {train_r2:.2f}, Validation R²: {valid_r2:.2f}\")\n",
        "    print(f\"Training MSE: {train_mse:.2f}, Validation MSE: {valid_mse:.2f}\")\n",
        "\n",
        "    if train_r2 > 0.8 and (train_r2 - valid_r2) > 0.1 and valid_mse > 2 * train_mse:\n",
        "        print(\"Overfitting detected: High training performance, but poor validation performance.\")\n",
        "        return \"overfitting\"\n",
        "    elif train_r2 < 0.6 and valid_r2 < 0.6:\n",
        "        print(\"Underfitting detected: Poor performance on both training and validation sets.\")\n",
        "        return \"underfitting\"\n",
        "    elif abs(train_r2 - valid_r2) < 0.1 and train_r2 > 0.7:\n",
        "        print(\"Good performance: Similar and high performance on both sets.\")\n",
        "        return \"good\"\n",
        "    else:\n",
        "        print(\"Moderate performance: Model may need slight tuning.\")\n",
        "        return \"moderate\"\n",
        "\n",
        "# Step 4: Train Ideal Packaging Model\n",
        "def train_ideal_packaging_model(X_train, y_train, X_valid, y_valid):\n",
        "    print(\"\\nTraining ideal packaging model with XGBoost...\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "    # Define XGBoost model for GridSearchCV (without early stopping)\n",
        "    xgb = XGBRegressor(random_state=42)\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [2, 3],\n",
        "        'learning_rate': [0.01, 0.05],\n",
        "        'subsample': [0.7, 0.9],\n",
        "        'reg_lambda': [1, 10],\n",
        "        'reg_alpha': [0, 1]\n",
        "    }\n",
        "    grid_search = GridSearchCV(\n",
        "        xgb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Get best parameters\n",
        "    best_params = grid_search.best_params_\n",
        "    print(f\"Best parameters from GridSearchCV: {best_params}\")\n",
        "\n",
        "    # Train final model with early stopping\n",
        "    final_model = XGBRegressor(\n",
        "        random_state=42,\n",
        "        n_estimators=best_params['n_estimators'],\n",
        "        max_depth=best_params['max_depth'],\n",
        "        learning_rate=best_params['learning_rate'],\n",
        "        subsample=best_params['subsample'],\n",
        "        reg_lambda=best_params['reg_lambda'],\n",
        "        reg_alpha=best_params['reg_alpha'],\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    final_model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        eval_set=[(X_valid_scaled, y_valid)],\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Evaluate on training set\n",
        "    y_pred_train = final_model.predict(X_train_scaled)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    print(f\"Ideal Packaging Model - Train MSE: {train_mse:.4f}, Train R²: {train_r2:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred_valid = final_model.predict(X_valid_scaled)\n",
        "    valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
        "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
        "    print(f\"Ideal Packaging Model - Validation MSE: {valid_mse:.4f}, Validation R²: {valid_r2:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    feature_names = ['dim_length', 'dim_width', 'dim_height']\n",
        "    importances = final_model.feature_importances_\n",
        "    for name, importance in zip(feature_names, importances):\n",
        "        print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
        "\n",
        "    return final_model, scaler, train_mse, train_r2, valid_mse, valid_r2\n",
        "\n",
        "# Step 5: Detect Flaws\n",
        "def detect_flaws(df_valid, model, scaler):\n",
        "    X_valid = df_valid[['dim_length', 'dim_width', 'dim_height']].values\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    df_valid['ideal_box_volume'] = model.predict(X_valid_scaled)\n",
        "    df_valid['ideal_product_size'] = df_valid['ideal_box_volume'] * 0.9\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid['design_flaw'] = df_valid['product_size'] > 2 * df_valid['ideal_product_size']\n",
        "    df_valid['packaging_flaw'] = df_valid['box_volume'] > 1.5 * df_valid['ideal_box_volume']\n",
        "\n",
        "    # Excess volume calculation\n",
        "    df_valid['excess_volume'] = df_valid.apply(\n",
        "        lambda row: row['box_volume'] - row['ideal_box_volume'] if row['packaging_flaw'] else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Recommendation\n",
        "    df_valid['recommendation'] = df_valid.apply(\n",
        "        lambda row: (\n",
        "            f\"Reduce box to {row['ideal_box_volume']:.1f} cm³ (save {row['excess_volume']:.1f} cm³)\"\n",
        "            if row['packaging_flaw']\n",
        "            else \"Redesign bottle to reduce volume\" if row['design_flaw']\n",
        "            else \"No flaw detected\"\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Outlier analysis\n",
        "    print(\"\\nValidation Set Outlier Analysis:\")\n",
        "    Q1 = df_valid['box_volume'].quantile(0.25)\n",
        "    Q3 = df_valid['box_volume'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df_valid[df_valid['box_volume'] > Q3 + 1.5 * IQR]\n",
        "    if not outliers.empty:\n",
        "        print(f\"Found {len(outliers)} potential outliers in validation set 'box_volume':\")\n",
        "        print(outliers[['ENTITY_NAME', 'box_volume', 'product_size']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No outliers detected in validation set 'box_volume'.\")\n",
        "\n",
        "    return df_valid\n",
        "\n",
        "# Step 6: Main Execution\n",
        "def main():\n",
        "    install_dependencies()\n",
        "\n",
        "    file_path = \"amazon_shampoo_products.csv\"\n",
        "    df = preprocess_data(file_path)\n",
        "\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Split data\n",
        "    df_train, df_valid = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
        "    print(f\"Training set size: {len(df_train)} records, Validation set size: {len(df_valid)} records\")\n",
        "\n",
        "    # Prepare data for model\n",
        "    features = ['dim_length', 'dim_width', 'dim_height']\n",
        "    X_train = df_train[features].values\n",
        "    y_train = df_train['box_volume']\n",
        "    X_valid = df_valid[features].values\n",
        "    y_valid = df_valid['box_volume']\n",
        "\n",
        "    # Train model\n",
        "    model, scaler, train_mse, train_r2, valid_mse, valid_r2 = train_ideal_packaging_model(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    # Diagnose model\n",
        "    diagnose_result = diagnose_model(train_r2, valid_r2, train_mse, valid_mse)\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid = detect_flaws(df_valid, model, scaler)\n",
        "\n",
        "    # Save results\n",
        "    output_df = df_valid[[\n",
        "        'ENTITY_ID', 'ENTITY_NAME', 'ENTITY_LENGTH', 'product_size', 'box_volume',\n",
        "        'ideal_product_size', 'ideal_box_volume', 'design_flaw', 'packaging_flaw',\n",
        "        'excess_volume', 'recommendation'\n",
        "    ]]\n",
        "    output_file = \"packaging_output_v5.csv\"\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nResults saved to '{output_file}'.\")\n",
        "\n",
        "    # Summary\n",
        "    design_flaws = df_valid['design_flaw'].sum()\n",
        "    packaging_flaws = df_valid['packaging_flaw'].sum()\n",
        "    print(f\"Detected {design_flaws} products with design flaws.\")\n",
        "    print(f\"Detected {packaging_flaws} products with packaging flaws.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6ghyUT_u2TI",
        "outputId": "66b35165-5cd9-4526-c64f-d6a968ae9f48"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed.\n",
            "Loading data...\n",
            "After removing duplicates: 140 rows\n",
            "After outlier removal: 113 rows\n",
            "Dropped 0 rows with NaN in key features. Remaining: 113\n",
            "Training set size: 79 records, Validation set size: 34 records\n",
            "\n",
            "Training ideal packaging model with XGBoost...\n",
            "Best parameters from GridSearchCV: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 0.7}\n",
            "[0]\tvalidation_0-rmse:719.70322\n",
            "[1]\tvalidation_0-rmse:707.43252\n",
            "[2]\tvalidation_0-rmse:686.23796\n",
            "[3]\tvalidation_0-rmse:670.87738\n",
            "[4]\tvalidation_0-rmse:655.49900\n",
            "[5]\tvalidation_0-rmse:641.92388\n",
            "[6]\tvalidation_0-rmse:623.65725\n",
            "[7]\tvalidation_0-rmse:611.83847\n",
            "[8]\tvalidation_0-rmse:601.47821\n",
            "[9]\tvalidation_0-rmse:594.26910\n",
            "[10]\tvalidation_0-rmse:585.17352\n",
            "[11]\tvalidation_0-rmse:578.66095\n",
            "[12]\tvalidation_0-rmse:569.59035\n",
            "[13]\tvalidation_0-rmse:561.07561\n",
            "[14]\tvalidation_0-rmse:547.72931\n",
            "[15]\tvalidation_0-rmse:542.98100\n",
            "[16]\tvalidation_0-rmse:531.03565\n",
            "[17]\tvalidation_0-rmse:524.68602\n",
            "[18]\tvalidation_0-rmse:518.84300\n",
            "[19]\tvalidation_0-rmse:507.16863\n",
            "[20]\tvalidation_0-rmse:502.25239\n",
            "[21]\tvalidation_0-rmse:498.17078\n",
            "[22]\tvalidation_0-rmse:484.54131\n",
            "[23]\tvalidation_0-rmse:472.69125\n",
            "[24]\tvalidation_0-rmse:465.05834\n",
            "[25]\tvalidation_0-rmse:452.79898\n",
            "[26]\tvalidation_0-rmse:450.24730\n",
            "[27]\tvalidation_0-rmse:447.93758\n",
            "[28]\tvalidation_0-rmse:438.54255\n",
            "[29]\tvalidation_0-rmse:428.46276\n",
            "[30]\tvalidation_0-rmse:424.28148\n",
            "[31]\tvalidation_0-rmse:420.05010\n",
            "[32]\tvalidation_0-rmse:417.38977\n",
            "[33]\tvalidation_0-rmse:413.95075\n",
            "[34]\tvalidation_0-rmse:411.85609\n",
            "[35]\tvalidation_0-rmse:404.98620\n",
            "[36]\tvalidation_0-rmse:401.82849\n",
            "[37]\tvalidation_0-rmse:400.52738\n",
            "[38]\tvalidation_0-rmse:398.40667\n",
            "[39]\tvalidation_0-rmse:394.89557\n",
            "[40]\tvalidation_0-rmse:386.75482\n",
            "[41]\tvalidation_0-rmse:383.49117\n",
            "[42]\tvalidation_0-rmse:377.78756\n",
            "[43]\tvalidation_0-rmse:371.28927\n",
            "[44]\tvalidation_0-rmse:367.95957\n",
            "[45]\tvalidation_0-rmse:362.07411\n",
            "[46]\tvalidation_0-rmse:360.42117\n",
            "[47]\tvalidation_0-rmse:354.08539\n",
            "[48]\tvalidation_0-rmse:352.10793\n",
            "[49]\tvalidation_0-rmse:347.49292\n",
            "[50]\tvalidation_0-rmse:343.09483\n",
            "[51]\tvalidation_0-rmse:337.82379\n",
            "[52]\tvalidation_0-rmse:335.79232\n",
            "[53]\tvalidation_0-rmse:332.50832\n",
            "[54]\tvalidation_0-rmse:331.07490\n",
            "[55]\tvalidation_0-rmse:328.86057\n",
            "[56]\tvalidation_0-rmse:324.58390\n",
            "[57]\tvalidation_0-rmse:319.76569\n",
            "[58]\tvalidation_0-rmse:317.26007\n",
            "[59]\tvalidation_0-rmse:315.02196\n",
            "[60]\tvalidation_0-rmse:313.59750\n",
            "[61]\tvalidation_0-rmse:312.06925\n",
            "[62]\tvalidation_0-rmse:310.42980\n",
            "[63]\tvalidation_0-rmse:306.96041\n",
            "[64]\tvalidation_0-rmse:305.34231\n",
            "[65]\tvalidation_0-rmse:303.83429\n",
            "[66]\tvalidation_0-rmse:300.11098\n",
            "[67]\tvalidation_0-rmse:296.94248\n",
            "[68]\tvalidation_0-rmse:296.11732\n",
            "[69]\tvalidation_0-rmse:293.87860\n",
            "[70]\tvalidation_0-rmse:290.20430\n",
            "[71]\tvalidation_0-rmse:289.30742\n",
            "[72]\tvalidation_0-rmse:285.78800\n",
            "[73]\tvalidation_0-rmse:284.50461\n",
            "[74]\tvalidation_0-rmse:282.01772\n",
            "[75]\tvalidation_0-rmse:281.15714\n",
            "[76]\tvalidation_0-rmse:279.58180\n",
            "[77]\tvalidation_0-rmse:277.03972\n",
            "[78]\tvalidation_0-rmse:275.66575\n",
            "[79]\tvalidation_0-rmse:274.91699\n",
            "[80]\tvalidation_0-rmse:273.15253\n",
            "[81]\tvalidation_0-rmse:271.23557\n",
            "[82]\tvalidation_0-rmse:269.45913\n",
            "[83]\tvalidation_0-rmse:268.31283\n",
            "[84]\tvalidation_0-rmse:266.42850\n",
            "[85]\tvalidation_0-rmse:266.05785\n",
            "[86]\tvalidation_0-rmse:265.10871\n",
            "[87]\tvalidation_0-rmse:263.55737\n",
            "[88]\tvalidation_0-rmse:263.25409\n",
            "[89]\tvalidation_0-rmse:262.60172\n",
            "[90]\tvalidation_0-rmse:262.26757\n",
            "[91]\tvalidation_0-rmse:260.86378\n",
            "[92]\tvalidation_0-rmse:259.65158\n",
            "[93]\tvalidation_0-rmse:257.48839\n",
            "[94]\tvalidation_0-rmse:256.58982\n",
            "[95]\tvalidation_0-rmse:255.63704\n",
            "[96]\tvalidation_0-rmse:253.54341\n",
            "[97]\tvalidation_0-rmse:253.39077\n",
            "[98]\tvalidation_0-rmse:251.62012\n",
            "[99]\tvalidation_0-rmse:250.49444\n",
            "Ideal Packaging Model - Train MSE: 17167.1807, Train R²: 0.9832\n",
            "Ideal Packaging Model - Validation MSE: 62747.4722, Validation R²: 0.8796\n",
            "Feature: dim_length, Importance: 0.1670\n",
            "Feature: dim_width, Importance: 0.6346\n",
            "Feature: dim_height, Importance: 0.1984\n",
            "\n",
            "Model Performance Diagnosis:\n",
            "Training R²: 0.98, Validation R²: 0.88\n",
            "Training MSE: 17167.18, Validation MSE: 62747.47\n",
            "Overfitting detected: High training performance, but poor validation performance.\n",
            "\n",
            "Validation Set Outlier Analysis:\n",
            "Found 1 potential outliers in validation set 'box_volume':\n",
            "                                                                                                                                       ENTITY_NAME  box_volume  product_size\n",
            "Dabur Vatika Ayurvedic Shampoo - 1L | Damage Therapy | With Power of 10 ingredients for solving 10 hair problems| No Parabens | For all hair types    3521.826        2646.0\n",
            "\n",
            "Results saved to 'packaging_output_v5.csv'.\n",
            "Detected 0 products with design flaws.\n",
            "Detected 0 products with packaging flaws.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Step 1: Install Dependencies\n",
        "def install_dependencies():\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q pandas numpy scikit-learn xgboost\")\n",
        "    print(\"Dependencies installed.\")\n",
        "\n",
        "# Step 2: Load and Preprocess Data\n",
        "def preprocess_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['ENTITY_NAME', 'ENTITY_LENGTH', 'DIMENSIONS', 'PRICE', 'BRAND'])\n",
        "    print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "    # Parse DIMENSIONS safely\n",
        "    df['dimensions'] = df['DIMENSIONS'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Impute missing DIMENSIONS with brand-specific medians\n",
        "    def impute_missing(group):\n",
        "        valid_dims = group['dimensions'].apply(\n",
        "            lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "        )\n",
        "        valid_dims_list = [x for x in valid_dims if x is not None]\n",
        "        if valid_dims_list:\n",
        "            median_dims = np.median(valid_dims_list, axis=0)\n",
        "        else:\n",
        "            all_valid_dims = df['dimensions'].apply(\n",
        "                lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "            )\n",
        "            median_dims = np.median([x for x in all_valid_dims if x is not None], axis=0)\n",
        "\n",
        "        group['dimensions'] = group['dimensions'].apply(\n",
        "            lambda x: tuple(median_dims) if x is None or not (isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x)) else x\n",
        "        )\n",
        "        return group\n",
        "\n",
        "    df = df.groupby('BRAND').apply(impute_missing, include_groups=False).reset_index()\n",
        "\n",
        "    # Extract individual dimensions\n",
        "    df['dim_length'] = df['dimensions'].apply(lambda x: x[0])\n",
        "    df['dim_width'] = df['dimensions'].apply(lambda x: x[1])\n",
        "    df['dim_height'] = df['dimensions'].apply(lambda x: x[2])\n",
        "\n",
        "    # Compute product_size\n",
        "    df['product_size'] = df['dimensions'].apply(\n",
        "        lambda x: x[0] * x[1] * x[2] if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Compute box_volume (add 10% padding to each dimension)\n",
        "    df['box_volume'] = df['dimensions'].apply(\n",
        "        lambda x: (x[0] * 1.1) * (x[1] * 1.1) * (x[2] * 1.1) if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Feature engineering: interaction term\n",
        "    df['dim_interaction'] = df['dim_length'] * df['dim_width'] * df['dim_height']\n",
        "\n",
        "    # Filter outliers in product_size (2*IQR)\n",
        "    Q1 = df['product_size'].quantile(0.25)\n",
        "    Q3 = df['product_size'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[df['product_size'].between(Q1 - 2 * IQR, Q3 + 2 * IQR)]\n",
        "    print(f\"After outlier removal: {len(df)} rows\")\n",
        "\n",
        "    # Drop rows with NaN in key features\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['dim_length', 'dim_width', 'dim_height', 'PRICE', 'product_size', 'dim_interaction'])\n",
        "    print(f\"Dropped {original_len - len(df)} rows with NaN in key features. Remaining: {len(df)}\")\n",
        "\n",
        "    if len(df) < 30:\n",
        "        print(f\"Error: Only {len(df)} valid records available. Need at least 30.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Diagnose Model Performance\n",
        "def diagnose_model(train_r2, valid_r2, train_mse, valid_mse):\n",
        "    print(\"\\nModel Performance Diagnosis:\")\n",
        "    print(f\"Training R²: {train_r2:.2f}, Validation R²: {valid_r2:.2f}\")\n",
        "    print(f\"Training MSE: {train_mse:.2f}, Validation MSE: {valid_mse:.2f}\")\n",
        "\n",
        "    if train_r2 > 0.8 and (train_r2 - valid_r2) > 0.1 and valid_mse > 2 * train_mse:\n",
        "        print(\"Overfitting detected: High training performance, but poor validation performance.\")\n",
        "        return \"overfitting\"\n",
        "    elif train_r2 < 0.6 and valid_r2 < 0.6:\n",
        "        print(\"Underfitting detected: Poor performance on both training and validation sets.\")\n",
        "        return \"underfitting\"\n",
        "    elif abs(train_r2 - valid_r2) < 0.1 and train_r2 > 0.7:\n",
        "        print(\"Good performance: Similar and high performance on both sets.\")\n",
        "        return \"good\"\n",
        "    else:\n",
        "        print(\"Moderate performance: Model may need slight tuning.\")\n",
        "        return \"moderate\"\n",
        "\n",
        "# Step 4: Train Ideal Packaging Model\n",
        "def train_ideal_packaging_model(X_train, y_train, X_valid, y_valid):\n",
        "    print(\"\\nTraining ideal packaging model with XGBoost...\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "    # Define XGBoost model for GridSearchCV\n",
        "    xgb = XGBRegressor(random_state=42)\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [2, 3],\n",
        "        'learning_rate': [0.01],\n",
        "        'subsample': [0.7, 0.9],\n",
        "        'reg_lambda': [10, 20],\n",
        "        'reg_alpha': [1, 5]\n",
        "    }\n",
        "    grid_search = GridSearchCV(\n",
        "        xgb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Get best parameters\n",
        "    best_params = grid_search.best_params_\n",
        "    print(f\"Best parameters from GridSearchCV: {best_params}\")\n",
        "\n",
        "    # Train final model with early stopping\n",
        "    final_model = XGBRegressor(\n",
        "        random_state=42,\n",
        "        n_estimators=best_params['n_estimators'],\n",
        "        max_depth=best_params['max_depth'],\n",
        "        learning_rate=best_params['learning_rate'],\n",
        "        subsample=best_params['subsample'],\n",
        "        reg_lambda=best_params['reg_lambda'],\n",
        "        reg_alpha=best_params['reg_alpha'],\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    final_model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        eval_set=[(X_valid_scaled, y_valid)],\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Evaluate on training set\n",
        "    y_pred_train = final_model.predict(X_train_scaled)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    print(f\"Ideal Packaging Model - Train MSE: {train_mse:.4f}, Train R²: {train_r2:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred_valid = final_model.predict(X_valid_scaled)\n",
        "    valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
        "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
        "    print(f\"Ideal Packaging Model - Validation MSE: {valid_mse:.4f}, Validation R²: {valid_r2:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    feature_names = ['dim_length', 'dim_width', 'dim_height', 'PRICE', 'dim_interaction']\n",
        "    importances = final_model.feature_importances_\n",
        "    for name, importance in zip(feature_names, importances):\n",
        "        print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
        "\n",
        "    return final_model, scaler, train_mse, train_r2, valid_mse, valid_r2\n",
        "\n",
        "# Step 5: Detect Flaws\n",
        "def detect_flaws(df_valid, model, scaler):\n",
        "    X_valid = df_valid[['dim_length', 'dim_width', 'dim_height', 'PRICE', 'dim_interaction']].values\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    df_valid['ideal_box_volume'] = model.predict(X_valid_scaled)\n",
        "    df_valid['ideal_product_size'] = df_valid['ideal_box_volume'] * 0.9\n",
        "\n",
        "    # Detect flaws with adjusted threshold\n",
        "    df_valid['design_flaw'] = df_valid['product_size'] > 2 * df_valid['ideal_product_size']\n",
        "    df_valid['packaging_flaw'] = df_valid['box_volume'] > 1.3 * df_valid['ideal_box_volume']\n",
        "\n",
        "    # Excess volume calculation\n",
        "    df_valid['excess_volume'] = df_valid.apply(\n",
        "        lambda row: row['box_volume'] - row['ideal_box_volume'] if row['packaging_flaw'] else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Recommendation\n",
        "    df_valid['recommendation'] = df_valid.apply(\n",
        "        lambda row: (\n",
        "            f\"Reduce box to {row['ideal_box_volume']:.1f} cm^3 (save {row['excess_volume']:.1f} cm^3)\"\n",
        "            if row['packaging_flaw']\n",
        "            else \"Redesign bottle to reduce volume\" if row['design_flaw']\n",
        "            else \"No flaw detected\"\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Near-flaw analysis\n",
        "    print(\"\\nNear-Flaw Analysis (Packaging Flaw Threshold > 1.2 * ideal_box_volume):\")\n",
        "    near_flaws = df_valid[df_valid['box_volume'] > 1.2 * df_valid['ideal_box_volume']]\n",
        "    if not near_flaws.empty:\n",
        "        print(f\"Found {len(near_flaws)} products near packaging flaw threshold:\")\n",
        "        print(near_flaws[['ENTITY_NAME', 'box_volume', 'ideal_box_volume', 'recommendation']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No products near packaging flaw threshold.\")\n",
        "\n",
        "    # Outlier analysis\n",
        "    print(\"\\nValidation Set Outlier Analysis:\")\n",
        "    Q1 = df_valid['box_volume'].quantile(0.25)\n",
        "    Q3 = df_valid['box_volume'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df_valid[df_valid['box_volume'] > Q3 + 1.5 * IQR]\n",
        "    if not outliers.empty:\n",
        "        print(f\"Found {len(outliers)} potential outliers in validation set 'box_volume':\")\n",
        "        print(outliers[['ENTITY_NAME', 'box_volume', 'product_size']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No outliers detected in validation set 'box_volume'.\")\n",
        "\n",
        "    return df_valid\n",
        "\n",
        "# Step 6: Main Execution\n",
        "def main():\n",
        "    install_dependencies()\n",
        "\n",
        "    file_path = \"amazon_shampoo_products.csv\"\n",
        "    df = preprocess_data(file_path)\n",
        "\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Split data\n",
        "    df_train, df_valid = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
        "    print(f\"Training set size: {len(df_train)} records, Validation set size: {len(df_valid)} records\")\n",
        "\n",
        "    # Prepare data for model\n",
        "    features = ['dim_length', 'dim_width', 'dim_height', 'PRICE', 'dim_interaction']\n",
        "    X_train = df_train[features].values\n",
        "    y_train = df_train['box_volume']\n",
        "    X_valid = df_valid[features].values\n",
        "    y_valid = df_valid['box_volume']\n",
        "\n",
        "    # Train model\n",
        "    model, scaler, train_mse, train_r2, valid_mse, valid_r2 = train_ideal_packaging_model(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    # Diagnose model\n",
        "    diagnose_result = diagnose_model(train_r2, valid_r2, train_mse, valid_mse)\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid = detect_flaws(df_valid, model, scaler)\n",
        "\n",
        "    # Save results\n",
        "    output_df = df_valid[[\n",
        "        'ENTITY_ID', 'ENTITY_NAME', 'ENTITY_LENGTH', 'product_size', 'box_volume',\n",
        "        'ideal_product_size', 'ideal_box_volume', 'design_flaw', 'packaging_flaw',\n",
        "        'excess_volume', 'recommendation'\n",
        "    ]]\n",
        "    output_file = \"packaging_output_v6.csv\"\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nResults saved to '{output_file}'.\")\n",
        "\n",
        "    # Summary\n",
        "    design_flaws = df_valid['design_flaw'].sum()\n",
        "    packaging_flaws = df_valid['packaging_flaw'].sum()\n",
        "    print(f\"Detected {design_flaws} products with design flaws.\")\n",
        "    print(f\"Detected {packaging_flaws} products with packaging flaws.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjKITzWVvdEW",
        "outputId": "19d82bf1-4364-44d1-f3f6-23ed820a927d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed.\n",
            "Loading data...\n",
            "After removing duplicates: 140 rows\n",
            "After outlier removal: 113 rows\n",
            "Dropped 0 rows with NaN in key features. Remaining: 113\n",
            "Training set size: 79 records, Validation set size: 34 records\n",
            "\n",
            "Training ideal packaging model with XGBoost...\n",
            "Best parameters from GridSearchCV: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 1, 'reg_lambda': 10, 'subsample': 0.9}\n",
            "[0]\tvalidation_0-rmse:731.42848\n",
            "[1]\tvalidation_0-rmse:726.60949\n",
            "[2]\tvalidation_0-rmse:722.07058\n",
            "[3]\tvalidation_0-rmse:717.22366\n",
            "[4]\tvalidation_0-rmse:712.40280\n",
            "[5]\tvalidation_0-rmse:707.70724\n",
            "[6]\tvalidation_0-rmse:703.18972\n",
            "[7]\tvalidation_0-rmse:698.53448\n",
            "[8]\tvalidation_0-rmse:693.87938\n",
            "[9]\tvalidation_0-rmse:689.30922\n",
            "[10]\tvalidation_0-rmse:684.70345\n",
            "[11]\tvalidation_0-rmse:680.17712\n",
            "[12]\tvalidation_0-rmse:675.73152\n",
            "[13]\tvalidation_0-rmse:671.31035\n",
            "[14]\tvalidation_0-rmse:666.85178\n",
            "[15]\tvalidation_0-rmse:662.26282\n",
            "[16]\tvalidation_0-rmse:657.92962\n",
            "[17]\tvalidation_0-rmse:653.56484\n",
            "[18]\tvalidation_0-rmse:649.30870\n",
            "[19]\tvalidation_0-rmse:645.14193\n",
            "[20]\tvalidation_0-rmse:641.08620\n",
            "[21]\tvalidation_0-rmse:637.01837\n",
            "[22]\tvalidation_0-rmse:633.08113\n",
            "[23]\tvalidation_0-rmse:629.24010\n",
            "[24]\tvalidation_0-rmse:625.20284\n",
            "[25]\tvalidation_0-rmse:621.38627\n",
            "[26]\tvalidation_0-rmse:617.49863\n",
            "[27]\tvalidation_0-rmse:613.79511\n",
            "[28]\tvalidation_0-rmse:610.09760\n",
            "[29]\tvalidation_0-rmse:606.11753\n",
            "[30]\tvalidation_0-rmse:602.42890\n",
            "[31]\tvalidation_0-rmse:598.74829\n",
            "[32]\tvalidation_0-rmse:595.10189\n",
            "[33]\tvalidation_0-rmse:591.42146\n",
            "[34]\tvalidation_0-rmse:587.79766\n",
            "[35]\tvalidation_0-rmse:584.14088\n",
            "[36]\tvalidation_0-rmse:580.38431\n",
            "[37]\tvalidation_0-rmse:576.78250\n",
            "[38]\tvalidation_0-rmse:573.20676\n",
            "[39]\tvalidation_0-rmse:569.68359\n",
            "[40]\tvalidation_0-rmse:566.27835\n",
            "[41]\tvalidation_0-rmse:562.70917\n",
            "[42]\tvalidation_0-rmse:559.39779\n",
            "[43]\tvalidation_0-rmse:556.05605\n",
            "[44]\tvalidation_0-rmse:552.75915\n",
            "[45]\tvalidation_0-rmse:549.48079\n",
            "[46]\tvalidation_0-rmse:546.35614\n",
            "[47]\tvalidation_0-rmse:543.26295\n",
            "[48]\tvalidation_0-rmse:540.04544\n",
            "[49]\tvalidation_0-rmse:536.85570\n",
            "[50]\tvalidation_0-rmse:533.71452\n",
            "[51]\tvalidation_0-rmse:530.66662\n",
            "[52]\tvalidation_0-rmse:527.57648\n",
            "[53]\tvalidation_0-rmse:524.57853\n",
            "[54]\tvalidation_0-rmse:521.56771\n",
            "[55]\tvalidation_0-rmse:518.46102\n",
            "[56]\tvalidation_0-rmse:515.56008\n",
            "[57]\tvalidation_0-rmse:512.57762\n",
            "[58]\tvalidation_0-rmse:509.60780\n",
            "[59]\tvalidation_0-rmse:506.66764\n",
            "[60]\tvalidation_0-rmse:503.74327\n",
            "[61]\tvalidation_0-rmse:500.93253\n",
            "[62]\tvalidation_0-rmse:498.00319\n",
            "[63]\tvalidation_0-rmse:495.09242\n",
            "[64]\tvalidation_0-rmse:492.22413\n",
            "[65]\tvalidation_0-rmse:489.48705\n",
            "[66]\tvalidation_0-rmse:486.77580\n",
            "[67]\tvalidation_0-rmse:484.02401\n",
            "[68]\tvalidation_0-rmse:481.26715\n",
            "[69]\tvalidation_0-rmse:478.59993\n",
            "[70]\tvalidation_0-rmse:475.89644\n",
            "[71]\tvalidation_0-rmse:473.08818\n",
            "[72]\tvalidation_0-rmse:470.53438\n",
            "[73]\tvalidation_0-rmse:468.02344\n",
            "[74]\tvalidation_0-rmse:465.46955\n",
            "[75]\tvalidation_0-rmse:462.86024\n",
            "[76]\tvalidation_0-rmse:460.29374\n",
            "[77]\tvalidation_0-rmse:457.72053\n",
            "[78]\tvalidation_0-rmse:455.23583\n",
            "[79]\tvalidation_0-rmse:452.67850\n",
            "[80]\tvalidation_0-rmse:450.13579\n",
            "[81]\tvalidation_0-rmse:447.66610\n",
            "[82]\tvalidation_0-rmse:445.41047\n",
            "[83]\tvalidation_0-rmse:443.03460\n",
            "[84]\tvalidation_0-rmse:440.62307\n",
            "[85]\tvalidation_0-rmse:438.32823\n",
            "[86]\tvalidation_0-rmse:436.01665\n",
            "[87]\tvalidation_0-rmse:433.65014\n",
            "[88]\tvalidation_0-rmse:431.27699\n",
            "[89]\tvalidation_0-rmse:428.89797\n",
            "[90]\tvalidation_0-rmse:426.60119\n",
            "[91]\tvalidation_0-rmse:424.41691\n",
            "[92]\tvalidation_0-rmse:422.34864\n",
            "[93]\tvalidation_0-rmse:420.15537\n",
            "[94]\tvalidation_0-rmse:417.88693\n",
            "[95]\tvalidation_0-rmse:415.65582\n",
            "[96]\tvalidation_0-rmse:413.43217\n",
            "[97]\tvalidation_0-rmse:411.26146\n",
            "[98]\tvalidation_0-rmse:409.07955\n",
            "[99]\tvalidation_0-rmse:406.90205\n",
            "Ideal Packaging Model - Train MSE: 408007.3207, Train R²: 0.6002\n",
            "Ideal Packaging Model - Validation MSE: 165569.2881, Validation R²: 0.6823\n",
            "Feature: dim_length, Importance: 0.0013\n",
            "Feature: dim_width, Importance: 0.0011\n",
            "Feature: dim_height, Importance: 0.0045\n",
            "Feature: PRICE, Importance: 0.0000\n",
            "Feature: dim_interaction, Importance: 0.9930\n",
            "\n",
            "Model Performance Diagnosis:\n",
            "Training R²: 0.60, Validation R²: 0.68\n",
            "Training MSE: 408007.32, Validation MSE: 165569.29\n",
            "Moderate performance: Model may need slight tuning.\n",
            "\n",
            "Near-Flaw Analysis (Packaging Flaw Threshold > 1.2 * ideal_box_volume):\n",
            "Found 5 products near packaging flaw threshold:\n",
            "                                                                                                                                                 ENTITY_NAME  box_volume  ideal_box_volume                               recommendation\n",
            "          Dabur Vatika Ayurvedic Shampoo - 1L | Damage Therapy | With Power of 10 ingredients for solving 10 hair problems| No Parabens | For all hair types  3521.82600       1944.538574 Reduce box to 1944.5 cm^3 (save 1577.3 cm^3)\n",
            "                                            Sunsilk Argan Oil & Rosemary Frizz Smooth Oil Blends Shampoo | for Frizzy Hair | with No Added Parabens | 700 ML  1830.65740       1407.414062  Reduce box to 1407.4 cm^3 (save 423.2 cm^3)\n",
            "Vedix Ayurvedic Shampoo for Dry and Frizzy Hair, Vikleda Deep Conditioning Customized Sulfate Free Men Shampoo with Wheat Germ, Jojoba & Yashtimadhu, 100 ml  2427.74400       1944.538574                             No flaw detected\n",
            "        TRESemme Keratin Smooth Shampoo 340 ml, With Keratin & Argan Oil for Straighter, Shinier Hair - Nourishes Dry Hair & Controls Frizz, For Men & Women  1343.13872       1106.123657                             No flaw detected\n",
            "                                                                                                          Dove Hair Fall Rescue Shampoo For Weak Hair, 1 Ltr  2071.88784       1686.293335                             No flaw detected\n",
            "\n",
            "Validation Set Outlier Analysis:\n",
            "Found 1 potential outliers in validation set 'box_volume':\n",
            "                                                                                                                                       ENTITY_NAME  box_volume  product_size\n",
            "Dabur Vatika Ayurvedic Shampoo - 1L | Damage Therapy | With Power of 10 ingredients for solving 10 hair problems| No Parabens | For all hair types    3521.826        2646.0\n",
            "\n",
            "Results saved to 'packaging_output_v6.csv'.\n",
            "Detected 0 products with design flaws.\n",
            "Detected 2 products with packaging flaws.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Step 1: Install Dependencies\n",
        "def install_dependencies():\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q pandas numpy scikit-learn xgboost\")\n",
        "    print(\"Dependencies installed.\")\n",
        "\n",
        "# Step 2: Load and Preprocess Data\n",
        "def preprocess_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['ENTITY_NAME', 'ENTITY_LENGTH', 'DIMENSIONS', 'PRICE', 'BRAND'])\n",
        "    print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "    # Parse DIMENSIONS safely\n",
        "    df['dimensions'] = df['DIMENSIONS'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Impute missing DIMENSIONS and ENTITY_LENGTH with brand-specific medians\n",
        "    def impute_missing(group):\n",
        "        # Impute dimensions\n",
        "        valid_dims = group['dimensions'].apply(\n",
        "            lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "        )\n",
        "        valid_dims_list = [x for x in valid_dims if x is not None]\n",
        "        if valid_dims_list:\n",
        "            median_dims = np.median(valid_dims_list, axis=0)\n",
        "        else:\n",
        "            all_valid_dims = df['dimensions'].apply(\n",
        "                lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "            )\n",
        "            median_dims = np.median([x for x in all_valid_dims if x is not None], axis=0)\n",
        "\n",
        "        group['dimensions'] = group['dimensions'].apply(\n",
        "            lambda x: tuple(median_dims) if x is None or not (isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x)) else x\n",
        "        )\n",
        "\n",
        "        # Impute ENTITY_LENGTH\n",
        "        valid_lengths = group['ENTITY_LENGTH'].apply(\n",
        "            lambda x: x if isinstance(x, (int, float)) and not np.isnan(x) else None\n",
        "        )\n",
        "        valid_lengths_list = [x for x in valid_lengths if x is not None]\n",
        "        median_length = np.median(valid_lengths_list) if valid_lengths_list else df['ENTITY_LENGTH'].median()\n",
        "\n",
        "        group['ENTITY_LENGTH'] = group['ENTITY_LENGTH'].fillna(median_length)\n",
        "        return group\n",
        "\n",
        "    df = df.groupby('BRAND').apply(impute_missing, include_groups=False).reset_index()\n",
        "\n",
        "    # Extract individual dimensions\n",
        "    df['dim_length'] = df['dimensions'].apply(lambda x: x[0])\n",
        "    df['dim_width'] = df['dimensions'].apply(lambda x: x[1])\n",
        "    df['dim_height'] = df['dimensions'].apply(lambda x: x[2])\n",
        "\n",
        "    # Compute product_size\n",
        "    df['product_size'] = df['dimensions'].apply(\n",
        "        lambda x: x[0] * x[1] * x[2] if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Compute box_volume (add 10% padding to each dimension)\n",
        "    df['box_volume'] = df['dimensions'].apply(\n",
        "        lambda x: (x[0] * 1.1) * (x[1] * 1.1) * (x[2] * 1.1) if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Filter outliers in product_size (2*IQR)\n",
        "    Q1 = df['product_size'].quantile(0.25)\n",
        "    Q3 = df['product_size'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[df['product_size'].between(Q1 - 2 * IQR, Q3 + 2 * IQR)]\n",
        "    print(f\"After outlier removal: {len(df)} rows\")\n",
        "\n",
        "    # Drop rows with NaN in key features\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['dim_length', 'dim_width', 'dim_height', 'PRICE', 'ENTITY_LENGTH', 'product_size'])\n",
        "    print(f\"Dropped {original_len - len(df)} rows with NaN in key features. Remaining: {len(df)}\")\n",
        "\n",
        "    if len(df) < 30:\n",
        "        print(f\"Error: Only {len(df)} valid records available. Need at least 30.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Diagnose Model Performance\n",
        "def diagnose_model(train_r2, valid_r2, train_mse, valid_mse):\n",
        "    print(\"\\nModel Performance Diagnosis:\")\n",
        "    print(f\"Training R²: {train_r2:.2f}, Validation R²: {valid_r2:.2f}\")\n",
        "    print(f\"Training MSE: {train_mse:.2f}, Validation MSE: {valid_mse:.2f}\")\n",
        "\n",
        "    if train_r2 > 0.8 and (train_r2 - valid_r2) > 0.1 and valid_mse > 2 * train_mse:\n",
        "        print(\"Overfitting detected: High training performance, but poor validation performance.\")\n",
        "        return \"overfitting\"\n",
        "    elif train_r2 < 0.6 and valid_r2 < 0.6:\n",
        "        print(\"Underfitting detected: Poor performance on both training and validation sets.\")\n",
        "        return \"underfitting\"\n",
        "    elif abs(train_r2 - valid_r2) < 0.1 and train_r2 > 0.7:\n",
        "        print(\"Good performance: Similar and high performance on both sets.\")\n",
        "        return \"good\"\n",
        "    else:\n",
        "        print(\"Moderate performance: Model may need slight tuning.\")\n",
        "        return \"moderate\"\n",
        "\n",
        "# Step 4: Train Ideal Packaging Model\n",
        "def train_ideal_packaging_model(X_train, y_train, X_valid, y_valid):\n",
        "    print(\"\\nTraining ideal packaging model with XGBoost...\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "    # Define XGBoost model for GridSearchCV\n",
        "    xgb = XGBRegressor(random_state=42)\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [3, 4],\n",
        "        'learning_rate': [0.01, 0.05],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'reg_lambda': [1, 5],\n",
        "        'reg_alpha': [0, 1]\n",
        "    }\n",
        "    grid_search = GridSearchCV(\n",
        "        xgb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Get best parameters\n",
        "    best_params = grid_search.best_params_\n",
        "    print(f\"Best parameters from GridSearchCV: {best_params}\")\n",
        "\n",
        "    # Train final model with early stopping\n",
        "    final_model = XGBRegressor(\n",
        "        random_state=42,\n",
        "        n_estimators=best_params['n_estimators'],\n",
        "        max_depth=best_params['max_depth'],\n",
        "        learning_rate=best_params['learning_rate'],\n",
        "        subsample=best_params['subsample'],\n",
        "        reg_lambda=best_params['reg_lambda'],\n",
        "        reg_alpha=best_params['reg_alpha'],\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    final_model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        eval_set=[(X_valid_scaled, y_valid)],\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Evaluate on training set\n",
        "    y_pred_train = final_model.predict(X_train_scaled)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    print(f\"Ideal Packaging Model - Train MSE: {train_mse:.4f}, Train R²: {train_r2:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred_valid = final_model.predict(X_valid_scaled)\n",
        "    valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
        "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
        "    print(f\"Ideal Packaging Model - Validation MSE: {valid_mse:.4f}, Validation R²: {valid_r2:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    feature_names = ['dim_length', 'dim_width', 'dim_height', 'PRICE', 'ENTITY_LENGTH']\n",
        "    importances = final_model.feature_importances_\n",
        "    for name, importance in zip(feature_names, importances):\n",
        "        print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
        "\n",
        "    return final_model, scaler, train_mse, train_r2, valid_mse, valid_r2\n",
        "\n",
        "# Step 5: Detect Flaws\n",
        "def detect_flaws(df_valid, model, scaler):\n",
        "    X_valid = df_valid[['dim_length', 'dim_width', 'dim_height', 'PRICE', 'ENTITY_LENGTH']].values\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    df_valid['ideal_box_volume'] = model.predict(X_valid_scaled)\n",
        "    df_valid['ideal_product_size'] = df_valid['ideal_box_volume'] * 0.9\n",
        "\n",
        "    # Detect flaws with adjusted thresholds\n",
        "    df_valid['design_flaw'] = df_valid['product_size'] > 1.5 * df_valid['ideal_product_size']\n",
        "    df_valid['packaging_flaw'] = df_valid['box_volume'] > 1.25 * df_valid['ideal_box_volume']\n",
        "\n",
        "    # Excess volume calculation\n",
        "    df_valid['excess_volume'] = df_valid.apply(\n",
        "        lambda row: row['box_volume'] - row['ideal_box_volume'] if row['packaging_flaw'] else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Recommendation\n",
        "    df_valid['recommendation'] = df_valid.apply(\n",
        "        lambda row: (\n",
        "            f\"Reduce box to {row['ideal_box_volume']:.1f} cm³ (save {row['excess_volume']:.1f} cm³)\"\n",
        "            if row['packaging_flaw']\n",
        "            else \"Redesign bottle to reduce volume\" if row['design_flaw']\n",
        "            else \"No flaw detected\"\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Near-flaw analysis\n",
        "    print(\"\\nNear-Flaw Analysis (Packaging Flaw Threshold > 1.1 * ideal_box_volume, Design Flaw > 1.3 * ideal_product_size):\")\n",
        "    near_packaging_flaws = df_valid[df_valid['box_volume'] > 1.1 * df_valid['ideal_box_volume']]\n",
        "    near_design_flaws = df_valid[df_valid['product_size'] > 1.3 * df_valid['ideal_product_size']]\n",
        "    if not near_packaging_flaws.empty:\n",
        "        print(f\"Found {len(near_packaging_flaws)} products near packaging flaw threshold:\")\n",
        "        print(near_packaging_flaws[['ENTITY_NAME', 'box_volume', 'ideal_box_volume', 'recommendation']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No products near packaging flaw threshold.\")\n",
        "    if not near_design_flaws.empty:\n",
        "        print(f\"Found {len(near_design_flaws)} products near design flaw threshold:\")\n",
        "        print(near_design_flaws[['ENTITY_NAME', 'product_size', 'ideal_product_size', 'recommendation']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No products near design flaw threshold.\")\n",
        "\n",
        "    # Outlier analysis\n",
        "    print(\"\\nValidation Set Outlier Analysis:\")\n",
        "    Q1 = df_valid['box_volume'].quantile(0.25)\n",
        "    Q3 = df_valid['box_volume'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df_valid[df_valid['box_volume'] > Q3 + 1.5 * IQR]\n",
        "    if not outliers.empty:\n",
        "        print(f\"Found {len(outliers)} potential outliers in validation set 'box_volume':\")\n",
        "        print(outliers[['ENTITY_NAME', 'box_volume', 'product_size']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No outliers detected in validation set 'box_volume'.\")\n",
        "\n",
        "    # Top 5 closest to flaw thresholds\n",
        "    df_valid['packaging_flaw_ratio'] = df_valid['box_volume'] / df_valid['ideal_box_volume']\n",
        "    df_valid['design_flaw_ratio'] = df_valid['product_size'] / df_valid['ideal_product_size']\n",
        "    print(\"\\nTop 5 Products Closest to Packaging Flaw Threshold:\")\n",
        "    print(df_valid.nlargest(5, 'packaging_flaw_ratio')[['ENTITY_NAME', 'box_volume', 'ideal_box_volume', 'packaging_flaw_ratio', 'recommendation']].to_string(index=False))\n",
        "    print(\"\\nTop 5 Products Closest to Design Flaw Threshold:\")\n",
        "    print(df_valid.nlargest(5, 'design_flaw_ratio')[['ENTITY_NAME', 'product_size', 'ideal_product_size', 'design_flaw_ratio', 'recommendation']].to_string(index=False))\n",
        "\n",
        "    return df_valid\n",
        "\n",
        "# Step 6: Main Execution\n",
        "def main():\n",
        "    install_dependencies()\n",
        "\n",
        "    file_path = \"amazon_shampoo_products.csv\"\n",
        "    df = preprocess_data(file_path)\n",
        "\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Split data\n",
        "    df_train, df_valid = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
        "    print(f\"Training set size: {len(df_train)} records, Validation set size: {len(df_valid)} records\")\n",
        "\n",
        "    # Prepare data for model\n",
        "    features = ['dim_length', 'dim_width', 'dim_height', 'PRICE', 'ENTITY_LENGTH']\n",
        "    X_train = df_train[features].values\n",
        "    y_train = df_train['box_volume']\n",
        "    X_valid = df_valid[features].values\n",
        "    y_valid = df_valid['box_volume']\n",
        "\n",
        "    # Train model\n",
        "    model, scaler, train_mse, train_r2, valid_mse, valid_r2 = train_ideal_packaging_model(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    # Diagnose model\n",
        "    diagnose_result = diagnose_model(train_r2, valid_r2, train_mse, valid_mse)\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid = detect_flaws(df_valid, model, scaler)\n",
        "\n",
        "    # Save results\n",
        "    output_df = df_valid[[\n",
        "        'ENTITY_ID', 'ENTITY_NAME', 'ENTITY_LENGTH', 'product_size', 'box_volume',\n",
        "        'ideal_product_size', 'ideal_box_volume', 'design_flaw', 'packaging_flaw',\n",
        "        'excess_volume', 'recommendation'\n",
        "    ]]\n",
        "    output_file = \"packaging_output_v7.csv\"\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nResults saved to '{output_file}'.\")\n",
        "\n",
        "    # Summary\n",
        "    design_flaws = df_valid['design_flaw'].sum()\n",
        "    packaging_flaws = df_valid['packaging_flaw'].sum()\n",
        "    print(f\"Detected {design_flaws} products with design flaws.\")\n",
        "    print(f\"Detected {packaging_flaws} products with packaging flaws.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww8Mnmdh6P3V",
        "outputId": "77ed6024-1160-48f1-d2cb-a398d8235e3b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed.\n",
            "Loading data...\n",
            "After removing duplicates: 140 rows\n",
            "After outlier removal: 113 rows\n",
            "Dropped 2 rows with NaN in key features. Remaining: 111\n",
            "Training set size: 77 records, Validation set size: 34 records\n",
            "\n",
            "Training ideal packaging model with XGBoost...\n",
            "Best parameters from GridSearchCV: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8}\n",
            "[0]\tvalidation_0-rmse:654.79059\n",
            "[1]\tvalidation_0-rmse:631.59668\n",
            "[2]\tvalidation_0-rmse:608.72113\n",
            "[3]\tvalidation_0-rmse:592.85743\n",
            "[4]\tvalidation_0-rmse:582.24928\n",
            "[5]\tvalidation_0-rmse:565.38899\n",
            "[6]\tvalidation_0-rmse:550.90567\n",
            "[7]\tvalidation_0-rmse:537.60775\n",
            "[8]\tvalidation_0-rmse:525.63691\n",
            "[9]\tvalidation_0-rmse:513.65535\n",
            "[10]\tvalidation_0-rmse:499.95905\n",
            "[11]\tvalidation_0-rmse:493.11347\n",
            "[12]\tvalidation_0-rmse:482.69153\n",
            "[13]\tvalidation_0-rmse:472.27721\n",
            "[14]\tvalidation_0-rmse:463.32477\n",
            "[15]\tvalidation_0-rmse:452.97010\n",
            "[16]\tvalidation_0-rmse:446.38316\n",
            "[17]\tvalidation_0-rmse:438.98534\n",
            "[18]\tvalidation_0-rmse:430.68384\n",
            "[19]\tvalidation_0-rmse:425.08810\n",
            "[20]\tvalidation_0-rmse:419.61381\n",
            "[21]\tvalidation_0-rmse:413.81334\n",
            "[22]\tvalidation_0-rmse:404.48830\n",
            "[23]\tvalidation_0-rmse:395.91250\n",
            "[24]\tvalidation_0-rmse:390.47704\n",
            "[25]\tvalidation_0-rmse:387.13757\n",
            "[26]\tvalidation_0-rmse:383.68661\n",
            "[27]\tvalidation_0-rmse:373.35878\n",
            "[28]\tvalidation_0-rmse:369.83994\n",
            "[29]\tvalidation_0-rmse:365.10469\n",
            "[30]\tvalidation_0-rmse:359.79772\n",
            "[31]\tvalidation_0-rmse:354.75229\n",
            "[32]\tvalidation_0-rmse:351.17973\n",
            "[33]\tvalidation_0-rmse:346.33091\n",
            "[34]\tvalidation_0-rmse:341.92059\n",
            "[35]\tvalidation_0-rmse:335.53468\n",
            "[36]\tvalidation_0-rmse:330.89445\n",
            "[37]\tvalidation_0-rmse:325.62049\n",
            "[38]\tvalidation_0-rmse:321.87285\n",
            "[39]\tvalidation_0-rmse:319.52565\n",
            "[40]\tvalidation_0-rmse:316.65933\n",
            "[41]\tvalidation_0-rmse:314.81781\n",
            "[42]\tvalidation_0-rmse:310.29494\n",
            "[43]\tvalidation_0-rmse:307.01481\n",
            "[44]\tvalidation_0-rmse:304.49480\n",
            "[45]\tvalidation_0-rmse:302.04616\n",
            "[46]\tvalidation_0-rmse:297.33180\n",
            "[47]\tvalidation_0-rmse:294.34904\n",
            "[48]\tvalidation_0-rmse:291.67131\n",
            "[49]\tvalidation_0-rmse:289.77069\n",
            "[50]\tvalidation_0-rmse:286.16903\n",
            "[51]\tvalidation_0-rmse:283.74285\n",
            "[52]\tvalidation_0-rmse:281.32446\n",
            "[53]\tvalidation_0-rmse:279.05092\n",
            "[54]\tvalidation_0-rmse:277.57279\n",
            "[55]\tvalidation_0-rmse:274.16161\n",
            "[56]\tvalidation_0-rmse:271.68897\n",
            "[57]\tvalidation_0-rmse:269.91157\n",
            "[58]\tvalidation_0-rmse:268.22908\n",
            "[59]\tvalidation_0-rmse:266.75558\n",
            "[60]\tvalidation_0-rmse:265.17734\n",
            "[61]\tvalidation_0-rmse:264.02771\n",
            "[62]\tvalidation_0-rmse:262.86932\n",
            "[63]\tvalidation_0-rmse:261.74388\n",
            "[64]\tvalidation_0-rmse:259.77605\n",
            "[65]\tvalidation_0-rmse:258.92491\n",
            "[66]\tvalidation_0-rmse:257.99594\n",
            "[67]\tvalidation_0-rmse:257.18453\n",
            "[68]\tvalidation_0-rmse:256.51937\n",
            "[69]\tvalidation_0-rmse:255.88323\n",
            "[70]\tvalidation_0-rmse:254.27306\n",
            "[71]\tvalidation_0-rmse:251.24122\n",
            "[72]\tvalidation_0-rmse:250.38323\n",
            "[73]\tvalidation_0-rmse:249.79144\n",
            "[74]\tvalidation_0-rmse:247.43315\n",
            "[75]\tvalidation_0-rmse:246.63650\n",
            "[76]\tvalidation_0-rmse:245.74553\n",
            "[77]\tvalidation_0-rmse:245.20990\n",
            "[78]\tvalidation_0-rmse:244.81541\n",
            "[79]\tvalidation_0-rmse:243.96226\n",
            "[80]\tvalidation_0-rmse:243.43950\n",
            "[81]\tvalidation_0-rmse:243.30912\n",
            "[82]\tvalidation_0-rmse:242.98296\n",
            "[83]\tvalidation_0-rmse:242.55943\n",
            "[84]\tvalidation_0-rmse:241.64135\n",
            "[85]\tvalidation_0-rmse:240.29878\n",
            "[86]\tvalidation_0-rmse:240.02861\n",
            "[87]\tvalidation_0-rmse:239.64172\n",
            "[88]\tvalidation_0-rmse:238.95750\n",
            "[89]\tvalidation_0-rmse:238.57347\n",
            "[90]\tvalidation_0-rmse:237.92066\n",
            "[91]\tvalidation_0-rmse:237.14473\n",
            "[92]\tvalidation_0-rmse:236.34417\n",
            "[93]\tvalidation_0-rmse:236.08254\n",
            "[94]\tvalidation_0-rmse:236.21775\n",
            "[95]\tvalidation_0-rmse:235.98031\n",
            "[96]\tvalidation_0-rmse:236.32948\n",
            "[97]\tvalidation_0-rmse:235.09390\n",
            "[98]\tvalidation_0-rmse:234.27413\n",
            "[99]\tvalidation_0-rmse:233.52774\n",
            "[100]\tvalidation_0-rmse:233.41509\n",
            "[101]\tvalidation_0-rmse:233.21409\n",
            "[102]\tvalidation_0-rmse:233.26275\n",
            "[103]\tvalidation_0-rmse:233.39771\n",
            "[104]\tvalidation_0-rmse:232.22194\n",
            "[105]\tvalidation_0-rmse:231.98259\n",
            "[106]\tvalidation_0-rmse:231.63409\n",
            "[107]\tvalidation_0-rmse:231.78191\n",
            "[108]\tvalidation_0-rmse:231.22671\n",
            "[109]\tvalidation_0-rmse:230.95633\n",
            "[110]\tvalidation_0-rmse:229.75332\n",
            "[111]\tvalidation_0-rmse:229.07065\n",
            "[112]\tvalidation_0-rmse:228.90487\n",
            "[113]\tvalidation_0-rmse:229.01778\n",
            "[114]\tvalidation_0-rmse:228.57218\n",
            "[115]\tvalidation_0-rmse:228.64360\n",
            "[116]\tvalidation_0-rmse:228.54931\n",
            "[117]\tvalidation_0-rmse:228.02828\n",
            "[118]\tvalidation_0-rmse:228.10191\n",
            "[119]\tvalidation_0-rmse:228.21432\n",
            "[120]\tvalidation_0-rmse:228.16558\n",
            "[121]\tvalidation_0-rmse:228.12309\n",
            "[122]\tvalidation_0-rmse:228.00312\n",
            "[123]\tvalidation_0-rmse:228.20221\n",
            "[124]\tvalidation_0-rmse:227.83704\n",
            "[125]\tvalidation_0-rmse:227.25929\n",
            "[126]\tvalidation_0-rmse:226.93270\n",
            "[127]\tvalidation_0-rmse:227.16111\n",
            "[128]\tvalidation_0-rmse:227.00387\n",
            "[129]\tvalidation_0-rmse:227.04239\n",
            "[130]\tvalidation_0-rmse:226.87305\n",
            "[131]\tvalidation_0-rmse:226.65557\n",
            "[132]\tvalidation_0-rmse:226.07346\n",
            "[133]\tvalidation_0-rmse:225.64094\n",
            "[134]\tvalidation_0-rmse:225.62160\n",
            "[135]\tvalidation_0-rmse:225.01990\n",
            "[136]\tvalidation_0-rmse:225.27814\n",
            "[137]\tvalidation_0-rmse:225.34135\n",
            "[138]\tvalidation_0-rmse:225.02207\n",
            "[139]\tvalidation_0-rmse:224.90629\n",
            "[140]\tvalidation_0-rmse:224.57369\n",
            "[141]\tvalidation_0-rmse:224.21262\n",
            "[142]\tvalidation_0-rmse:223.87255\n",
            "[143]\tvalidation_0-rmse:224.11450\n",
            "[144]\tvalidation_0-rmse:224.18280\n",
            "[145]\tvalidation_0-rmse:223.88463\n",
            "[146]\tvalidation_0-rmse:224.04403\n",
            "[147]\tvalidation_0-rmse:224.09677\n",
            "[148]\tvalidation_0-rmse:224.10669\n",
            "[149]\tvalidation_0-rmse:223.95799\n",
            "[150]\tvalidation_0-rmse:223.94415\n",
            "[151]\tvalidation_0-rmse:223.62428\n",
            "[152]\tvalidation_0-rmse:223.55893\n",
            "[153]\tvalidation_0-rmse:223.35934\n",
            "[154]\tvalidation_0-rmse:223.31507\n",
            "[155]\tvalidation_0-rmse:223.25279\n",
            "[156]\tvalidation_0-rmse:223.28485\n",
            "[157]\tvalidation_0-rmse:223.27895\n",
            "[158]\tvalidation_0-rmse:223.48013\n",
            "[159]\tvalidation_0-rmse:223.28890\n",
            "[160]\tvalidation_0-rmse:223.17656\n",
            "[161]\tvalidation_0-rmse:222.93452\n",
            "[162]\tvalidation_0-rmse:222.82553\n",
            "[163]\tvalidation_0-rmse:222.56823\n",
            "[164]\tvalidation_0-rmse:222.47951\n",
            "[165]\tvalidation_0-rmse:222.02920\n",
            "[166]\tvalidation_0-rmse:221.38096\n",
            "[167]\tvalidation_0-rmse:220.95965\n",
            "[168]\tvalidation_0-rmse:220.93135\n",
            "[169]\tvalidation_0-rmse:220.73064\n",
            "[170]\tvalidation_0-rmse:220.66265\n",
            "[171]\tvalidation_0-rmse:220.64544\n",
            "[172]\tvalidation_0-rmse:220.51915\n",
            "[173]\tvalidation_0-rmse:220.46672\n",
            "[174]\tvalidation_0-rmse:220.36768\n",
            "[175]\tvalidation_0-rmse:220.30564\n",
            "[176]\tvalidation_0-rmse:220.32493\n",
            "[177]\tvalidation_0-rmse:220.22992\n",
            "[178]\tvalidation_0-rmse:220.16872\n",
            "[179]\tvalidation_0-rmse:219.77423\n",
            "[180]\tvalidation_0-rmse:219.69260\n",
            "[181]\tvalidation_0-rmse:219.50441\n",
            "[182]\tvalidation_0-rmse:219.44053\n",
            "[183]\tvalidation_0-rmse:219.29337\n",
            "[184]\tvalidation_0-rmse:219.21420\n",
            "[185]\tvalidation_0-rmse:219.27443\n",
            "[186]\tvalidation_0-rmse:219.36045\n",
            "[187]\tvalidation_0-rmse:219.39488\n",
            "[188]\tvalidation_0-rmse:219.02682\n",
            "[189]\tvalidation_0-rmse:219.03446\n",
            "[190]\tvalidation_0-rmse:219.07907\n",
            "[191]\tvalidation_0-rmse:218.90934\n",
            "[192]\tvalidation_0-rmse:218.84030\n",
            "[193]\tvalidation_0-rmse:218.75316\n",
            "[194]\tvalidation_0-rmse:218.79251\n",
            "[195]\tvalidation_0-rmse:218.76645\n",
            "[196]\tvalidation_0-rmse:218.71173\n",
            "[197]\tvalidation_0-rmse:218.57170\n",
            "[198]\tvalidation_0-rmse:218.51113\n",
            "[199]\tvalidation_0-rmse:218.48463\n",
            "Ideal Packaging Model - Train MSE: 622.8539, Train R²: 0.9994\n",
            "Ideal Packaging Model - Validation MSE: 47735.5326, Validation R²: 0.8934\n",
            "Feature: dim_length, Importance: 0.0942\n",
            "Feature: dim_width, Importance: 0.7002\n",
            "Feature: dim_height, Importance: 0.1282\n",
            "Feature: PRICE, Importance: 0.0262\n",
            "Feature: ENTITY_LENGTH, Importance: 0.0512\n",
            "\n",
            "Model Performance Diagnosis:\n",
            "Training R²: 1.00, Validation R²: 0.89\n",
            "Training MSE: 622.85, Validation MSE: 47735.53\n",
            "Overfitting detected: High training performance, but poor validation performance.\n",
            "\n",
            "Near-Flaw Analysis (Packaging Flaw Threshold > 1.1 * ideal_box_volume, Design Flaw > 1.3 * ideal_product_size):\n",
            "Found 9 products near packaging flaw threshold:\n",
            "                                                                                                                                                                          ENTITY_NAME  box_volume  ideal_box_volume                           recommendation\n",
            "                                       Naturali Rice Water Shampoo for Frizzy and Dry Hair | With Rice Water | Sulphate Free | For Women | Clarifying Shampoo | Paraben Free | 200 ml   17.968500        -50.275116  Reduce box to -50.3 cm³ (save 68.2 cm³)\n",
            "                                                       Park Avenue Beer shampoo for Shiny & Bouncy Hair (650ml) | Paraben Free | For Dull & Lifeless Hair | Crafted with Natural Beer 1921.964000       1633.357788                         No flaw detected\n",
            "           L'Oreal Paris Anti-Hair Fall Shampoo, Reinforcing & Nourishing for Hair Growth, For Thinning & Hair Loss, With Arginine Essence and Salicylic Acid, Fall Resist 3X, 180 ml 1107.817920        812.457458 Reduce box to 812.5 cm³ (save 295.4 cm³)\n",
            "                     Moxie Beauty Shampoo For Frizzy, Dry, Wavy & Curly Hair | Sulfate, Paraben & Silicone-Free | 100% softer & 1.5x stronger hair | Gentle Cleansing Shampoo | 200ml  266.200000        213.752182                         No flaw detected\n",
            "ABSO ESSENTIALS Strengthening Shampoo for Frizz Control, Hair Strengthening, Enhanced Shine Infused with Vegan Keratin, Pro Vitamin B5 and Turmeric| Sulphate & Paraben Free (300 ML)  639.103608        574.813354                         No flaw detected\n",
            "                                                                         Mamaearth BhringAmla Shampoo for dry & frizzy hair with Bhringraj & Amla for Intense Hair Treatment – 250 ml  108.002664         77.878616   Reduce box to 77.9 cm³ (save 30.1 cm³)\n",
            "                                                                                                                                       Dove Daily Shine Shampoo For Dull Hair, 650 ml 2370.244800       1956.509399                         No flaw detected\n",
            "                                 TRESemme Keratin Smooth Shampoo 340 ml, With Keratin & Argan Oil for Straighter, Shinier Hair - Nourishes Dry Hair & Controls Frizz, For Men & Women 1343.138720        979.902954 Reduce box to 979.9 cm³ (save 363.2 cm³)\n",
            "                                                                             L'Oreal Paris Conditioner, For Damaged and Weak Hair, With Pro-Keratin + Ceramide, Total Repair 5, 180ml  665.380104        559.762451                         No flaw detected\n",
            "Found 1 products near design flaw threshold:\n",
            "                                                                                                                                   ENTITY_NAME  product_size  ideal_product_size                          recommendation\n",
            "Naturali Rice Water Shampoo for Frizzy and Dry Hair | With Rice Water | Sulphate Free | For Women | Clarifying Shampoo | Paraben Free | 200 ml          13.5          -45.247604 Reduce box to -50.3 cm³ (save 68.2 cm³)\n",
            "\n",
            "Validation Set Outlier Analysis:\n",
            "No outliers detected in validation set 'box_volume'.\n",
            "\n",
            "Top 5 Products Closest to Packaging Flaw Threshold:\n",
            "                                                                                                                                                               ENTITY_NAME  box_volume  ideal_box_volume  packaging_flaw_ratio                           recommendation\n",
            "                                                              Mamaearth BhringAmla Shampoo for dry & frizzy hair with Bhringraj & Amla for Intense Hair Treatment – 250 ml  108.002664         77.878616              1.386808   Reduce box to 77.9 cm³ (save 30.1 cm³)\n",
            "                      TRESemme Keratin Smooth Shampoo 340 ml, With Keratin & Argan Oil for Straighter, Shinier Hair - Nourishes Dry Hair & Controls Frizz, For Men & Women 1343.138720        979.902954              1.370685 Reduce box to 979.9 cm³ (save 363.2 cm³)\n",
            "L'Oreal Paris Anti-Hair Fall Shampoo, Reinforcing & Nourishing for Hair Growth, For Thinning & Hair Loss, With Arginine Essence and Salicylic Acid, Fall Resist 3X, 180 ml 1107.817920        812.457458              1.363540 Reduce box to 812.5 cm³ (save 295.4 cm³)\n",
            "          Moxie Beauty Shampoo For Frizzy, Dry, Wavy & Curly Hair | Sulfate, Paraben & Silicone-Free | 100% softer & 1.5x stronger hair | Gentle Cleansing Shampoo | 200ml  266.200000        213.752182              1.245367                         No flaw detected\n",
            "                                                                                                                            Dove Daily Shine Shampoo For Dull Hair, 650 ml 2370.244800       1956.509399              1.211466                         No flaw detected\n",
            "\n",
            "Top 5 Products Closest to Design Flaw Threshold:\n",
            "                                                                                                                                                               ENTITY_NAME  product_size  ideal_product_size  design_flaw_ratio                           recommendation\n",
            "                                                              Mamaearth BhringAmla Shampoo for dry & frizzy hair with Bhringraj & Amla for Intense Hair Treatment – 250 ml        81.144           70.090752           1.157699   Reduce box to 77.9 cm³ (save 30.1 cm³)\n",
            "                      TRESemme Keratin Smooth Shampoo 340 ml, With Keratin & Argan Oil for Straighter, Shinier Hair - Nourishes Dry Hair & Controls Frizz, For Men & Women      1009.120          881.912659           1.144240 Reduce box to 979.9 cm³ (save 363.2 cm³)\n",
            "L'Oreal Paris Anti-Hair Fall Shampoo, Reinforcing & Nourishing for Hair Growth, For Thinning & Hair Loss, With Arginine Essence and Salicylic Acid, Fall Resist 3X, 180 ml       832.320          731.211670           1.138275 Reduce box to 812.5 cm³ (save 295.4 cm³)\n",
            "          Moxie Beauty Shampoo For Frizzy, Dry, Wavy & Curly Hair | Sulfate, Paraben & Silicone-Free | 100% softer & 1.5x stronger hair | Gentle Cleansing Shampoo | 200ml       200.000          192.376953           1.039626                         No flaw detected\n",
            "                                                                                                                            Dove Daily Shine Shampoo For Dull Hair, 650 ml      1780.800         1760.858398           1.011325                         No flaw detected\n",
            "\n",
            "Results saved to 'packaging_output_v7.csv'.\n",
            "Detected 1 products with design flaws.\n",
            "Detected 4 products with packaging flaws.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# Step 1: Install Dependencies\n",
        "def install_dependencies():\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q pandas numpy scikit-learn xgboost\")\n",
        "    print(\"Dependencies installed.\")\n",
        "\n",
        "# Step 2: Load and Preprocess Data\n",
        "def preprocess_data(file_path):\n",
        "    print(\"Loading data...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['ENTITY_NAME', 'ENTITY_LENGTH', 'DIMENSIONS', 'PRICE', 'BRAND'])\n",
        "    print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "    # Parse DIMENSIONS safely\n",
        "    df['dimensions'] = df['DIMENSIONS'].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Impute missing DIMENSIONS and ENTITY_LENGTH with brand-specific medians\n",
        "    def impute_missing(group):\n",
        "        # Impute dimensions\n",
        "        valid_dims = group['dimensions'].apply(\n",
        "            lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "        )\n",
        "        valid_dims_list = [x for x in valid_dims if x is not None]\n",
        "        if valid_dims_list:\n",
        "            median_dims = np.median(valid_dims_list, axis=0)\n",
        "        else:\n",
        "            all_valid_dims = df['dimensions'].apply(\n",
        "                lambda x: x if isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x) else None\n",
        "            )\n",
        "            median_dims = np.median([x for x in all_valid_dims if x is not None], axis=0)\n",
        "\n",
        "        group['dimensions'] = group['dimensions'].apply(\n",
        "            lambda x: tuple(median_dims) if x is None or not (isinstance(x, tuple) and len(x) == 3 and all(isinstance(v, (int, float)) for v in x)) else x\n",
        "        )\n",
        "\n",
        "        # Impute ENTITY_LENGTH\n",
        "        valid_lengths = group['ENTITY_LENGTH'].apply(\n",
        "            lambda x: x if isinstance(x, (int, float)) and not np.isnan(x) else None\n",
        "        )\n",
        "        valid_lengths_list = [x for x in valid_lengths if x is not None]\n",
        "        median_length = np.median(valid_lengths_list) if valid_lengths_list else df['ENTITY_LENGTH'].median()\n",
        "\n",
        "        group['ENTITY_LENGTH'] = group['ENTITY_LENGTH'].fillna(median_length)\n",
        "        return group\n",
        "\n",
        "    df = df.groupby('BRAND').apply(impute_missing, include_groups=False).reset_index()\n",
        "\n",
        "    # Extract individual dimensions\n",
        "    df['dim_length'] = df['dimensions'].apply(lambda x: x[0])\n",
        "    df['dim_width'] = df['dimensions'].apply(lambda x: x[1])\n",
        "    df['dim_height'] = df['dimensions'].apply(lambda x: x[2])\n",
        "\n",
        "    # Compute product_size\n",
        "    df['product_size'] = df['dimensions'].apply(\n",
        "        lambda x: x[0] * x[1] * x[2] if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Compute box_volume (add 10% padding to each dimension)\n",
        "    df['box_volume'] = df['dimensions'].apply(\n",
        "        lambda x: (x[0] * 1.1) * (x[1] * 1.1) * (x[2] * 1.1) if all(isinstance(v, (int, float)) for v in x) else np.nan\n",
        "    )\n",
        "\n",
        "    # Filter outliers in product_size (2*IQR)\n",
        "    Q1 = df['product_size'].quantile(0.25)\n",
        "    Q3 = df['product_size'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[df['product_size'].between(Q1 - 2 * IQR, Q3 + 2 * IQR)]\n",
        "    print(f\"After outlier removal: {len(df)} rows\")\n",
        "\n",
        "    # Drop rows with NaN in key features\n",
        "    original_len = len(df)\n",
        "    df = df.dropna(subset=['dim_length', 'dim_width', 'dim_height', 'PRICE', 'ENTITY_LENGTH', 'product_size'])\n",
        "    print(f\"Dropped {original_len - len(df)} rows with NaN in key features. Remaining: {len(df)}\")\n",
        "\n",
        "    if len(df) < 30:\n",
        "        print(f\"Error: Only {len(df)} valid records available. Need at least 30.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 3: Diagnose Model Performance\n",
        "def diagnose_model(train_r2, valid_r2, train_mse, valid_mse):\n",
        "    print(\"\\nModel Performance Diagnosis:\")\n",
        "    print(f\"Training R²: {train_r2:.2f}, Validation R²: {valid_r2:.2f}\")\n",
        "    print(f\"Training MSE: {train_mse:.2f}, Validation MSE: {valid_mse:.2f}\")\n",
        "\n",
        "    if train_r2 > 0.8 and (train_r2 - valid_r2) > 0.1 and valid_mse > 2 * train_mse:\n",
        "        print(\"Overfitting detected: High training performance, but poor validation performance.\")\n",
        "        return \"overfitting\"\n",
        "    elif train_r2 < 0.6 and valid_r2 < 0.6:\n",
        "        print(\"Underfitting detected: Poor performance on both training and validation sets.\")\n",
        "        return \"underfitting\"\n",
        "    elif abs(train_r2 - valid_r2) < 0.1 and train_r2 > 0.7:\n",
        "        print(\"Good performance: Similar and high performance on both sets.\")\n",
        "        return \"good\"\n",
        "    else:\n",
        "        print(\"Moderate performance: Model may need slight tuning.\")\n",
        "        return \"moderate\"\n",
        "\n",
        "# Step 4: Train Ideal Packaging Model\n",
        "def train_ideal_packaging_model(X_train, y_train, X_valid, y_valid):\n",
        "    print(\"\\nTraining ideal packaging model with XGBoost...\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "\n",
        "    # Define XGBoost model for GridSearchCV\n",
        "    xgb = XGBRegressor(random_state=42, early_stopping_rounds=10)\n",
        "    param_grid = {\n",
        "        'max_depth': [2, 3],\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'learning_rate': [0.01, 0.05],\n",
        "        'subsample': [0.7, 0.8, 0.9],\n",
        "        'reg_lambda': [1, 5, 10],\n",
        "        'reg_alpha': [0, 1, 5],\n",
        "        'colsample_bytree': [0.7, 0.9]\n",
        "    }\n",
        "    grid_search = GridSearchCV(\n",
        "        xgb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n",
        "    )\n",
        "    grid_search.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        eval_set=[(X_valid_scaled, y_valid)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Get best parameters\n",
        "    best_params = grid_search.best_params_\n",
        "    print(f\"Best parameters from GridSearchCV: {best_params}\")\n",
        "\n",
        "    # Train final model with best parameters\n",
        "    final_model = XGBRegressor(\n",
        "        random_state=42,\n",
        "        max_depth=best_params['max_depth'],\n",
        "        n_estimators=best_params['n_estimators'],\n",
        "        learning_rate=best_params['learning_rate'],\n",
        "        subsample=best_params['subsample'],\n",
        "        reg_lambda=best_params['reg_lambda'],\n",
        "        reg_alpha=best_params['reg_alpha'],\n",
        "        colsample_bytree=best_params['colsample_bytree'],\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    final_model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        eval_set=[(X_valid_scaled, y_valid)],\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Evaluate on training set\n",
        "    y_pred_train = final_model.predict(X_train_scaled)\n",
        "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    print(f\"Ideal Packaging Model - Train MSE: {train_mse:.4f}, Train R²: {train_r2:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred_valid = final_model.predict(X_valid_scaled)\n",
        "    valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
        "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
        "    print(f\"Ideal Packaging Model - Validation MSE: {valid_mse:.4f}, Validation R²: {valid_r2:.4f}\")\n",
        "\n",
        "    # Feature importance\n",
        "    feature_names = ['dim_length', 'dim_width', 'dim_height', 'PRICE', 'ENTITY_LENGTH']\n",
        "    importances = final_model.feature_importances_\n",
        "    for name, importance in zip(feature_names, importances):\n",
        "        print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
        "\n",
        "    return final_model, scaler, train_mse, train_r2, valid_mse, valid_r2\n",
        "\n",
        "# Step 5: Detect Flaws\n",
        "def detect_flaws(df_valid, model, scaler):\n",
        "    X_valid = df_valid[['dim_length', 'dim_width', 'dim_height', 'PRICE', 'ENTITY_LENGTH']].values\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    df_valid['ideal_box_volume'] = np.maximum(model.predict(X_valid_scaled), 0)  # Clip to avoid negative predictions\n",
        "    df_valid['ideal_product_size'] = df_valid['ideal_box_volume'] * 0.9\n",
        "\n",
        "    # Detect flaws with adjusted thresholds\n",
        "    df_valid['design_flaw'] = df_valid['product_size'] > 1.5 * df_valid['ideal_product_size']\n",
        "    df_valid['packaging_flaw'] = df_valid['box_volume'] > 1.25 * df_valid['ideal_box_volume']\n",
        "\n",
        "    # Excess volume calculation\n",
        "    df_valid['excess_volume'] = df_valid.apply(\n",
        "        lambda row: row['box_volume'] - row['ideal_box_volume'] if row['packaging_flaw'] else 0,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Recommendation\n",
        "    df_valid['recommendation'] = df_valid.apply(\n",
        "        lambda row: (\n",
        "            f\"Reduce box to {row['ideal_box_volume']:.1f} cm³ (save {row['excess_volume']:.1f} cm³)\"\n",
        "            if row['packaging_flaw']\n",
        "            else \"Redesign bottle to reduce volume\" if row['design_flaw']\n",
        "            else \"No flaw detected\"\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Near-flaw analysis\n",
        "    print(\"\\nNear-Flaw Analysis (Packaging Flaw Threshold > 1.1 * ideal_box_volume, Design Flaw > 1.3 * ideal_product_size):\")\n",
        "    near_packaging_flaws = df_valid[df_valid['box_volume'] > 1.1 * df_valid['ideal_box_volume']]\n",
        "    near_design_flaws = df_valid[df_valid['product_size'] > 1.3 * df_valid['ideal_product_size']]\n",
        "    if not near_packaging_flaws.empty:\n",
        "        print(f\"Found {len(near_packaging_flaws)} products near packaging flaw threshold:\")\n",
        "        print(near_packaging_flaws[['ENTITY_NAME', 'box_volume', 'ideal_box_volume', 'recommendation']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No products near packaging flaw threshold.\")\n",
        "    if not near_design_flaws.empty:\n",
        "        print(f\"Found {len(near_design_flaws)} products near design flaw threshold:\")\n",
        "        print(near_design_flaws[['ENTITY_NAME', 'product_size', 'ideal_product_size', 'recommendation']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No products near design flaw threshold.\")\n",
        "\n",
        "    # Outlier analysis\n",
        "    print(\"\\nValidation Set Outlier Analysis:\")\n",
        "    Q1 = df_valid['box_volume'].quantile(0.25)\n",
        "    Q3 = df_valid['box_volume'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df_valid[df_valid['box_volume'] > Q3 + 1.5 * IQR]\n",
        "    if not outliers.empty:\n",
        "        print(f\"Found {len(outliers)} potential outliers in validation set 'box_volume':\")\n",
        "        print(outliers[['ENTITY_NAME', 'box_volume', 'product_size']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No outliers detected in validation set 'box_volume'.\")\n",
        "\n",
        "    # Top 5 closest to flaw thresholds\n",
        "    df_valid['packaging_flaw_ratio'] = df_valid['box_volume'] / df_valid['ideal_box_volume']\n",
        "    df_valid['design_flaw_ratio'] = df_valid['product_size'] / df_valid['ideal_product_size']\n",
        "    print(\"\\nTop 5 Products Closest to Packaging Flaw Threshold:\")\n",
        "    print(df_valid.nlargest(5, 'packaging_flaw_ratio')[['ENTITY_NAME', 'box_volume', 'ideal_box_volume', 'packaging_flaw_ratio', 'recommendation']].to_string(index=False))\n",
        "    print(\"\\nTop 5 Products Closest to Design Flaw Threshold:\")\n",
        "    print(df_valid.nlargest(5, 'design_flaw_ratio')[['ENTITY_NAME', 'product_size', 'ideal_product_size', 'design_flaw_ratio', 'recommendation']].to_string(index=False))\n",
        "\n",
        "    return df_valid\n",
        "\n",
        "# Step 6: Main Execution\n",
        "def main():\n",
        "    install_dependencies()\n",
        "\n",
        "    file_path = \"amazon_shampoo_products.csv\"\n",
        "    df = preprocess_data(file_path)\n",
        "\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Split data\n",
        "    df_train, df_valid = train_test_split(df, train_size=0.7, test_size=0.3, random_state=42)\n",
        "    print(f\"Training set size: {len(df_train)} records, Validation set size: {len(df_valid)} records\")\n",
        "\n",
        "    # Prepare data for model\n",
        "    features = ['dim_length', 'dim_width', 'dim_height', 'PRICE', 'ENTITY_LENGTH']\n",
        "    X_train = df_train[features].values\n",
        "    y_train = df_train['box_volume']\n",
        "    X_valid = df_valid[features].values\n",
        "    y_valid = df_valid['box_volume']\n",
        "\n",
        "    # Train model\n",
        "    model, scaler, train_mse, train_r2, valid_mse, valid_r2 = train_ideal_packaging_model(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    # Diagnose model\n",
        "    diagnose_result = diagnose_model(train_r2, valid_r2, train_mse, valid_mse)\n",
        "\n",
        "    # Detect flaws\n",
        "    df_valid = detect_flaws(df_valid, model, scaler)\n",
        "\n",
        "    # Save results\n",
        "    output_df = df_valid[[\n",
        "        'ENTITY_ID', 'ENTITY_NAME', 'ENTITY_LENGTH', 'product_size', 'box_volume',\n",
        "        'ideal_product_size', 'ideal_box_volume', 'design_flaw', 'packaging_flaw',\n",
        "        'excess_volume', 'recommendation'\n",
        "    ]]\n",
        "    output_file = \"packaging_output_v8.csv\"\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nResults saved to '{output_file}'.\")\n",
        "\n",
        "    # Summary\n",
        "    design_flaws = df_valid['design_flaw'].sum()\n",
        "    packaging_flaws = df_valid['packaging_flaw'].sum()\n",
        "    print(f\"Detected {design_flaws} products with design flaws.\")\n",
        "    print(f\"Detected {packaging_flaws} products with packaging flaws.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYKWWo0h8BTc",
        "outputId": "fccdec76-9173-4979-f2a6-82afd04eb8bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed.\n",
            "Loading data...\n",
            "After removing duplicates: 140 rows\n",
            "After outlier removal: 113 rows\n",
            "Dropped 2 rows with NaN in key features. Remaining: 111\n",
            "Training set size: 77 records, Validation set size: 34 records\n",
            "\n",
            "Training ideal packaging model with XGBoost...\n",
            "Best parameters from GridSearchCV: {'colsample_bytree': 0.9, 'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 0.7}\n",
            "[0]\tvalidation_0-rmse:663.71626\n",
            "[1]\tvalidation_0-rmse:650.76857\n",
            "[2]\tvalidation_0-rmse:625.53973\n",
            "[3]\tvalidation_0-rmse:613.44421\n",
            "[4]\tvalidation_0-rmse:592.42930\n",
            "[5]\tvalidation_0-rmse:581.06331\n",
            "[6]\tvalidation_0-rmse:570.81988\n",
            "[7]\tvalidation_0-rmse:562.33700\n",
            "[8]\tvalidation_0-rmse:557.34860\n",
            "[9]\tvalidation_0-rmse:546.47565\n",
            "[10]\tvalidation_0-rmse:541.28576\n",
            "[11]\tvalidation_0-rmse:536.21744\n",
            "[12]\tvalidation_0-rmse:530.79215\n",
            "[13]\tvalidation_0-rmse:516.14253\n",
            "[14]\tvalidation_0-rmse:513.16012\n",
            "[15]\tvalidation_0-rmse:508.43033\n",
            "[16]\tvalidation_0-rmse:505.47246\n",
            "[17]\tvalidation_0-rmse:504.44438\n",
            "[18]\tvalidation_0-rmse:500.49332\n",
            "[19]\tvalidation_0-rmse:498.11089\n",
            "[20]\tvalidation_0-rmse:493.81745\n",
            "[21]\tvalidation_0-rmse:483.62923\n",
            "[22]\tvalidation_0-rmse:480.36901\n",
            "[23]\tvalidation_0-rmse:467.62094\n",
            "[24]\tvalidation_0-rmse:467.71044\n",
            "[25]\tvalidation_0-rmse:456.24952\n",
            "[26]\tvalidation_0-rmse:453.82102\n",
            "[27]\tvalidation_0-rmse:446.62187\n",
            "[28]\tvalidation_0-rmse:444.36130\n",
            "[29]\tvalidation_0-rmse:445.70004\n",
            "[30]\tvalidation_0-rmse:440.70954\n",
            "[31]\tvalidation_0-rmse:438.71087\n",
            "[32]\tvalidation_0-rmse:436.12588\n",
            "[33]\tvalidation_0-rmse:434.92419\n",
            "[34]\tvalidation_0-rmse:426.27145\n",
            "[35]\tvalidation_0-rmse:417.91059\n",
            "[36]\tvalidation_0-rmse:415.23907\n",
            "[37]\tvalidation_0-rmse:411.53961\n",
            "[38]\tvalidation_0-rmse:406.89599\n",
            "[39]\tvalidation_0-rmse:400.49570\n",
            "[40]\tvalidation_0-rmse:391.32014\n",
            "[41]\tvalidation_0-rmse:386.13698\n",
            "[42]\tvalidation_0-rmse:384.60539\n",
            "[43]\tvalidation_0-rmse:381.52940\n",
            "[44]\tvalidation_0-rmse:380.45310\n",
            "[45]\tvalidation_0-rmse:379.51091\n",
            "[46]\tvalidation_0-rmse:374.48087\n",
            "[47]\tvalidation_0-rmse:372.82122\n",
            "[48]\tvalidation_0-rmse:371.60868\n",
            "[49]\tvalidation_0-rmse:371.81159\n",
            "[50]\tvalidation_0-rmse:369.81973\n",
            "[51]\tvalidation_0-rmse:366.96335\n",
            "[52]\tvalidation_0-rmse:362.65306\n",
            "[53]\tvalidation_0-rmse:360.47128\n",
            "[54]\tvalidation_0-rmse:361.89646\n",
            "[55]\tvalidation_0-rmse:357.41846\n",
            "[56]\tvalidation_0-rmse:355.22732\n",
            "[57]\tvalidation_0-rmse:353.91592\n",
            "[58]\tvalidation_0-rmse:349.64130\n",
            "[59]\tvalidation_0-rmse:348.43264\n",
            "[60]\tvalidation_0-rmse:347.27034\n",
            "[61]\tvalidation_0-rmse:343.79112\n",
            "[62]\tvalidation_0-rmse:342.48251\n",
            "[63]\tvalidation_0-rmse:339.25962\n",
            "[64]\tvalidation_0-rmse:335.91010\n",
            "[65]\tvalidation_0-rmse:335.00266\n",
            "[66]\tvalidation_0-rmse:333.05712\n",
            "[67]\tvalidation_0-rmse:331.22797\n",
            "[68]\tvalidation_0-rmse:329.97509\n",
            "[69]\tvalidation_0-rmse:329.46828\n",
            "[70]\tvalidation_0-rmse:328.99762\n",
            "[71]\tvalidation_0-rmse:325.43219\n",
            "[72]\tvalidation_0-rmse:324.46903\n",
            "[73]\tvalidation_0-rmse:324.05377\n",
            "[74]\tvalidation_0-rmse:322.34488\n",
            "[75]\tvalidation_0-rmse:321.55884\n",
            "[76]\tvalidation_0-rmse:320.52367\n",
            "[77]\tvalidation_0-rmse:319.04300\n",
            "[78]\tvalidation_0-rmse:318.25017\n",
            "[79]\tvalidation_0-rmse:315.54156\n",
            "[80]\tvalidation_0-rmse:315.65648\n",
            "[81]\tvalidation_0-rmse:313.65212\n",
            "[82]\tvalidation_0-rmse:312.70502\n",
            "[83]\tvalidation_0-rmse:312.26285\n",
            "[84]\tvalidation_0-rmse:311.30016\n",
            "[85]\tvalidation_0-rmse:310.89654\n",
            "[86]\tvalidation_0-rmse:310.59039\n",
            "[87]\tvalidation_0-rmse:310.27369\n",
            "[88]\tvalidation_0-rmse:309.09890\n",
            "[89]\tvalidation_0-rmse:308.58735\n",
            "[90]\tvalidation_0-rmse:306.90784\n",
            "[91]\tvalidation_0-rmse:307.15612\n",
            "[92]\tvalidation_0-rmse:306.85550\n",
            "[93]\tvalidation_0-rmse:305.54251\n",
            "[94]\tvalidation_0-rmse:303.74428\n",
            "[95]\tvalidation_0-rmse:303.03434\n",
            "[96]\tvalidation_0-rmse:303.13073\n",
            "[97]\tvalidation_0-rmse:302.31873\n",
            "[98]\tvalidation_0-rmse:301.45641\n",
            "[99]\tvalidation_0-rmse:301.11273\n",
            "[100]\tvalidation_0-rmse:300.12821\n",
            "[101]\tvalidation_0-rmse:297.82301\n",
            "[102]\tvalidation_0-rmse:297.97369\n",
            "[103]\tvalidation_0-rmse:297.16810\n",
            "[104]\tvalidation_0-rmse:294.46536\n",
            "[105]\tvalidation_0-rmse:294.23568\n",
            "[106]\tvalidation_0-rmse:294.64170\n",
            "[107]\tvalidation_0-rmse:293.78434\n",
            "[108]\tvalidation_0-rmse:293.31407\n",
            "[109]\tvalidation_0-rmse:293.39702\n",
            "[110]\tvalidation_0-rmse:291.48307\n",
            "[111]\tvalidation_0-rmse:291.42932\n",
            "[112]\tvalidation_0-rmse:291.08822\n",
            "[113]\tvalidation_0-rmse:290.18642\n",
            "[114]\tvalidation_0-rmse:290.31179\n",
            "[115]\tvalidation_0-rmse:290.06833\n",
            "[116]\tvalidation_0-rmse:290.35990\n",
            "[117]\tvalidation_0-rmse:290.26486\n",
            "[118]\tvalidation_0-rmse:289.99182\n",
            "[119]\tvalidation_0-rmse:289.72075\n",
            "[120]\tvalidation_0-rmse:289.51824\n",
            "[121]\tvalidation_0-rmse:288.06744\n",
            "[122]\tvalidation_0-rmse:287.69732\n",
            "[123]\tvalidation_0-rmse:287.79883\n",
            "[124]\tvalidation_0-rmse:287.34033\n",
            "[125]\tvalidation_0-rmse:285.57950\n",
            "[126]\tvalidation_0-rmse:285.35647\n",
            "[127]\tvalidation_0-rmse:285.36210\n",
            "[128]\tvalidation_0-rmse:285.03715\n",
            "[129]\tvalidation_0-rmse:284.18623\n",
            "[130]\tvalidation_0-rmse:283.23272\n",
            "[131]\tvalidation_0-rmse:282.65515\n",
            "[132]\tvalidation_0-rmse:282.49306\n",
            "[133]\tvalidation_0-rmse:281.83399\n",
            "[134]\tvalidation_0-rmse:281.66894\n",
            "[135]\tvalidation_0-rmse:280.94324\n",
            "[136]\tvalidation_0-rmse:280.82668\n",
            "[137]\tvalidation_0-rmse:280.79430\n",
            "[138]\tvalidation_0-rmse:280.42502\n",
            "[139]\tvalidation_0-rmse:280.11518\n",
            "[140]\tvalidation_0-rmse:279.78965\n",
            "[141]\tvalidation_0-rmse:279.58855\n",
            "[142]\tvalidation_0-rmse:278.92025\n",
            "[143]\tvalidation_0-rmse:278.96727\n",
            "[144]\tvalidation_0-rmse:278.72204\n",
            "[145]\tvalidation_0-rmse:278.98216\n",
            "[146]\tvalidation_0-rmse:278.39909\n",
            "[147]\tvalidation_0-rmse:278.30915\n",
            "[148]\tvalidation_0-rmse:278.30072\n",
            "[149]\tvalidation_0-rmse:278.51894\n",
            "[150]\tvalidation_0-rmse:278.59636\n",
            "[151]\tvalidation_0-rmse:278.93771\n",
            "[152]\tvalidation_0-rmse:278.71808\n",
            "[153]\tvalidation_0-rmse:278.58553\n",
            "[154]\tvalidation_0-rmse:278.31777\n",
            "[155]\tvalidation_0-rmse:277.56048\n",
            "[156]\tvalidation_0-rmse:277.21930\n",
            "[157]\tvalidation_0-rmse:276.60147\n",
            "[158]\tvalidation_0-rmse:276.68225\n",
            "[159]\tvalidation_0-rmse:276.16074\n",
            "[160]\tvalidation_0-rmse:276.22153\n",
            "[161]\tvalidation_0-rmse:276.14353\n",
            "[162]\tvalidation_0-rmse:276.01421\n",
            "[163]\tvalidation_0-rmse:275.85348\n",
            "[164]\tvalidation_0-rmse:275.73743\n",
            "[165]\tvalidation_0-rmse:275.07078\n",
            "[166]\tvalidation_0-rmse:275.16145\n",
            "[167]\tvalidation_0-rmse:275.12890\n",
            "[168]\tvalidation_0-rmse:275.21024\n",
            "[169]\tvalidation_0-rmse:275.17638\n",
            "[170]\tvalidation_0-rmse:275.33365\n",
            "[171]\tvalidation_0-rmse:275.12510\n",
            "[172]\tvalidation_0-rmse:275.05073\n",
            "[173]\tvalidation_0-rmse:274.88974\n",
            "[174]\tvalidation_0-rmse:274.01562\n",
            "[175]\tvalidation_0-rmse:273.91267\n",
            "[176]\tvalidation_0-rmse:273.85633\n",
            "[177]\tvalidation_0-rmse:273.22502\n",
            "[178]\tvalidation_0-rmse:272.56416\n",
            "[179]\tvalidation_0-rmse:272.43406\n",
            "[180]\tvalidation_0-rmse:272.49420\n",
            "[181]\tvalidation_0-rmse:272.83698\n",
            "[182]\tvalidation_0-rmse:272.42523\n",
            "[183]\tvalidation_0-rmse:272.50920\n",
            "[184]\tvalidation_0-rmse:271.90562\n",
            "[185]\tvalidation_0-rmse:271.94282\n",
            "[186]\tvalidation_0-rmse:271.92326\n",
            "[187]\tvalidation_0-rmse:271.61653\n",
            "[188]\tvalidation_0-rmse:271.68419\n",
            "[189]\tvalidation_0-rmse:271.68108\n",
            "[190]\tvalidation_0-rmse:272.41429\n",
            "[191]\tvalidation_0-rmse:272.47775\n",
            "[192]\tvalidation_0-rmse:272.34456\n",
            "[193]\tvalidation_0-rmse:272.23662\n",
            "[194]\tvalidation_0-rmse:272.50443\n",
            "[195]\tvalidation_0-rmse:272.46373\n",
            "[196]\tvalidation_0-rmse:272.54584\n",
            "[197]\tvalidation_0-rmse:272.66523\n",
            "Ideal Packaging Model - Train MSE: 3784.2327, Train R²: 0.9964\n",
            "Ideal Packaging Model - Validation MSE: 73775.5410, Validation R²: 0.8353\n",
            "Feature: dim_length, Importance: 0.1855\n",
            "Feature: dim_width, Importance: 0.4890\n",
            "Feature: dim_height, Importance: 0.1832\n",
            "Feature: PRICE, Importance: 0.0418\n",
            "Feature: ENTITY_LENGTH, Importance: 0.1006\n",
            "\n",
            "Model Performance Diagnosis:\n",
            "Training R²: 1.00, Validation R²: 0.84\n",
            "Training MSE: 3784.23, Validation MSE: 73775.54\n",
            "Overfitting detected: High training performance, but poor validation performance.\n",
            "\n",
            "Near-Flaw Analysis (Packaging Flaw Threshold > 1.1 * ideal_box_volume, Design Flaw > 1.3 * ideal_product_size):\n",
            "Found 10 products near packaging flaw threshold:\n",
            "                                                                                                                                                                       ENTITY_NAME  box_volume  ideal_box_volume                            recommendation\n",
            "                                    Naturali Rice Water Shampoo for Frizzy and Dry Hair | With Rice Water | Sulphate Free | For Women | Clarifying Shampoo | Paraben Free | 200 ml   17.968500          0.000000     Reduce box to 0.0 cm³ (save 18.0 cm³)\n",
            "                                                    Park Avenue Beer shampoo for Shiny & Bouncy Hair (650ml) | Paraben Free | For Dull & Lifeless Hair | Crafted with Natural Beer 1921.964000       1543.937866                          No flaw detected\n",
            "        L'Oreal Paris Anti-Hair Fall Shampoo, Reinforcing & Nourishing for Hair Growth, For Thinning & Hair Loss, With Arginine Essence and Salicylic Acid, Fall Resist 3X, 180 ml 1107.817920        809.686340  Reduce box to 809.7 cm³ (save 298.1 cm³)\n",
            "                                                                  Sunsilk Argan Oil & Rosemary Frizz Smooth Oil Blends Shampoo | for Frizzy Hair | with No Added Parabens | 700 ML 1830.657400       1636.993286                          No flaw detected\n",
            "                                                                      Mamaearth BhringAmla Shampoo for dry & frizzy hair with Bhringraj & Amla for Intense Hair Treatment – 250 ml  108.002664         21.423611    Reduce box to 21.4 cm³ (save 86.6 cm³)\n",
            "                                                                                                                                    Dove Daily Shine Shampoo For Dull Hair, 650 ml 2370.244800       1976.261841                          No flaw detected\n",
            "                              TRESemme Keratin Smooth Shampoo 340 ml, With Keratin & Argan Oil for Straighter, Shinier Hair - Nourishes Dry Hair & Controls Frizz, For Men & Women 1343.138720       1033.545288 Reduce box to 1033.5 cm³ (save 309.6 cm³)\n",
            "                                                                          L'Oreal Paris Conditioner, For Damaged and Weak Hair, With Pro-Keratin + Ceramide, Total Repair 5, 180ml  665.380104        581.160828                          No flaw detected\n",
            "                                                Bblunt Hair Fall Control Shampoo with Pea Protein & Caffeine for Stronger Hair | Reduces Up to 93% Hair Fall | Adds Shine | 300 ml 1483.479360       1322.987549                          No flaw detected\n",
            "Aroma Magic Moisture Boost Shampoo for Dry & chemical treated hair| with Argan Oil, Avocado & Grapefruit Extracts | Helps in Deep Moisturizing, Strengthen & Growth of hair- 200ml  479.160000        433.954346                          No flaw detected\n",
            "Found 2 products near design flaw threshold:\n",
            "                                                                                                                                   ENTITY_NAME  product_size  ideal_product_size                         recommendation\n",
            "Naturali Rice Water Shampoo for Frizzy and Dry Hair | With Rice Water | Sulphate Free | For Women | Clarifying Shampoo | Paraben Free | 200 ml        13.500             0.00000  Reduce box to 0.0 cm³ (save 18.0 cm³)\n",
            "                                  Mamaearth BhringAmla Shampoo for dry & frizzy hair with Bhringraj & Amla for Intense Hair Treatment – 250 ml        81.144            19.28125 Reduce box to 21.4 cm³ (save 86.6 cm³)\n",
            "\n",
            "Validation Set Outlier Analysis:\n",
            "No outliers detected in validation set 'box_volume'.\n",
            "\n",
            "Top 5 Products Closest to Packaging Flaw Threshold:\n",
            "                                                                                                                                                               ENTITY_NAME  box_volume  ideal_box_volume  packaging_flaw_ratio                            recommendation\n",
            "                            Naturali Rice Water Shampoo for Frizzy and Dry Hair | With Rice Water | Sulphate Free | For Women | Clarifying Shampoo | Paraben Free | 200 ml   17.968500          0.000000                   inf     Reduce box to 0.0 cm³ (save 18.0 cm³)\n",
            "                                                              Mamaearth BhringAmla Shampoo for dry & frizzy hair with Bhringraj & Amla for Intense Hair Treatment – 250 ml  108.002664         21.423611              5.041291    Reduce box to 21.4 cm³ (save 86.6 cm³)\n",
            "L'Oreal Paris Anti-Hair Fall Shampoo, Reinforcing & Nourishing for Hair Growth, For Thinning & Hair Loss, With Arginine Essence and Salicylic Acid, Fall Resist 3X, 180 ml 1107.817920        809.686340              1.368206  Reduce box to 809.7 cm³ (save 298.1 cm³)\n",
            "                      TRESemme Keratin Smooth Shampoo 340 ml, With Keratin & Argan Oil for Straighter, Shinier Hair - Nourishes Dry Hair & Controls Frizz, For Men & Women 1343.138720       1033.545288              1.299545 Reduce box to 1033.5 cm³ (save 309.6 cm³)\n",
            "                                            Park Avenue Beer shampoo for Shiny & Bouncy Hair (650ml) | Paraben Free | For Dull & Lifeless Hair | Crafted with Natural Beer 1921.964000       1543.937866              1.244845                          No flaw detected\n",
            "\n",
            "Top 5 Products Closest to Design Flaw Threshold:\n",
            "                                                                                                                                                               ENTITY_NAME  product_size  ideal_product_size  design_flaw_ratio                            recommendation\n",
            "                            Naturali Rice Water Shampoo for Frizzy and Dry Hair | With Rice Water | Sulphate Free | For Women | Clarifying Shampoo | Paraben Free | 200 ml        13.500            0.000000                inf     Reduce box to 0.0 cm³ (save 18.0 cm³)\n",
            "                                                              Mamaearth BhringAmla Shampoo for dry & frizzy hair with Bhringraj & Amla for Intense Hair Treatment – 250 ml        81.144           19.281250           4.208441    Reduce box to 21.4 cm³ (save 86.6 cm³)\n",
            "L'Oreal Paris Anti-Hair Fall Shampoo, Reinforcing & Nourishing for Hair Growth, For Thinning & Hair Loss, With Arginine Essence and Salicylic Acid, Fall Resist 3X, 180 ml       832.320          728.717712           1.142171  Reduce box to 809.7 cm³ (save 298.1 cm³)\n",
            "                      TRESemme Keratin Smooth Shampoo 340 ml, With Keratin & Argan Oil for Straighter, Shinier Hair - Nourishes Dry Hair & Controls Frizz, For Men & Women      1009.120          930.190735           1.084853 Reduce box to 1033.5 cm³ (save 309.6 cm³)\n",
            "                                            Park Avenue Beer shampoo for Shiny & Bouncy Hair (650ml) | Paraben Free | For Dull & Lifeless Hair | Crafted with Natural Beer      1444.000         1389.544067           1.039190                          No flaw detected\n",
            "\n",
            "Results saved to 'packaging_output_v8.csv'.\n",
            "Detected 2 products with design flaws.\n",
            "Detected 4 products with packaging flaws.\n"
          ]
        }
      ]
    }
  ]
}